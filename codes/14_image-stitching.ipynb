{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load images\n",
        "   - Link to the original image: [pexels.com/photo/areal-view-of-lake-bridge-and-trees-during-daytime-145525](https://www.pexels.com/photo/areal-view-of-lake-bridge-and-trees-during-daytime-145525/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# note: open-cv loads images in BGR format.\n",
        "scene_1 = cv2.imread('../assets/images/third_party/nature_1.jpg')\n",
        "scene_2 = cv2.imread('../assets/images/third_party/nature_2.jpg')\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
        "axs[0].imshow(cv2.cvtColor(scene_1, code=cv2.COLOR_BGR2RGB))\n",
        "axs[0].set_title(\"Scene 1\")\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(cv2.cvtColor(scene_2, code=cv2.COLOR_BGR2RGB))\n",
        "axs[1].set_title(\"Scene 2\")\n",
        "axs[1].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Stitching\n",
        "   - It is the process of combining multiple images to create a single larger image, often referred to as a panorama."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Key-Point detection using Scale-Invariant Feature Transform [SIFT]\n",
        "   - [SIFT](https://docs.opencv.org/4.x/da/df5/tutorial_py_sift_intro.html) is a corner detector like [Harris](https://docs.opencv.org/4.x/dc/d0d/tutorial_py_features_harris.html) (a key-point detector since corners are good candidates for key-points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BGR to GrayScale\n",
        "scene_1_gray = cv2.cvtColor(scene_1, cv2.COLOR_BGR2GRAY)\n",
        "scene_2_gray = cv2.cvtColor(scene_2, cv2.COLOR_BGR2GRAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a SIFT object\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "# detect keypoints and compute descriptors for the images\n",
        "keypoints_1, descriptors_1 = sift.detectAndCompute(scene_1, None)\n",
        "keypoints_2, descriptors_2 = sift.detectAndCompute(scene_2, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a new image including key-points\n",
        "scene_1_kp = cv2.drawKeypoints(scene_1, keypoints_1, None)\n",
        "scene_2_kp = cv2.drawKeypoints(scene_2, keypoints_2, None)\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n",
        "axs[0].imshow(cv2.cvtColor(scene_1_kp, cv2.COLOR_BGR2RGB))\n",
        "axs[0].set_title(f\"Reference [#keypoints:{descriptors_1.shape[0]} - #features per keypoint:{descriptors_1.shape[1]}]\")\n",
        "axs[0].axis('off')\n",
        "axs[1].imshow(cv2.cvtColor(scene_2_kp, cv2.COLOR_BGR2RGB))\n",
        "axs[1].set_title(f\"Target [#keypoints:{descriptors_2.shape[0]} - #features per keypoint:{descriptors_2.shape[1]}]\")\n",
        "axs[1].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature/Key-Point Matching with FLANN\n",
        "   - [cv2.FlannBasedMatcher](https://docs.opencv.org/4.x/d5/d6f/tutorial_feature_flann_matcher.html)\n",
        "   - [cv2.FlannBasedMatcher.knnMatch](https://docs.opencv.org/4.x/db/d39/classcv_1_1DescriptorMatcher.html#a378f35c9b1a5dfa4022839a45cdf0e89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# match the keypoints between the two images\n",
        "matcher = cv2.FlannBasedMatcher()\n",
        "matches = matcher.knnMatch(descriptors_1, descriptors_2, k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Apply thresholding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply ratio test to get good matches [thresholding]\n",
        "good_matches = []\n",
        "for m, n in matches:\n",
        "    if m.distance < 0.4 * n.distance:\n",
        "        good_matches.append(m)\n",
        "\n",
        "# create a new image showing the good matches\n",
        "matches_image = cv2.drawMatches(scene_1, keypoints_1, scene_2, keypoints_2, good_matches, None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.imshow(cv2.cvtColor(matches_image, cv2.COLOR_BGR2RGB))\n",
        "plt.axis('off')\n",
        "plt.title(f\"Number of matched key-points: {len(good_matches)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Homography\n",
        "   - [cv2.findHomography](https://docs.opencv.org/4.x/d7/dff/tutorial_feature_homography.html) is used to estimate a perspective transformation (homography) between two sets of corresponding points in different images.\n",
        "   - [cv2.perspectiveTransform](https://docs.opencv.org/4.x/d2/de8/group__core__array.html#gad327659ac03e5fd6894b90025e6900a7) performs the perspective matrix transformation of vectors.\n",
        "   - [cv2.warpPerspective](https://docs.opencv.org/4.x/da/d54/group__imgproc__transform.html#gaf73673a7e8e18ec6963e3774e6a94b87) applies a perspective transformation to an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_pts = np.float32([keypoints_1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "dst_pts = np.float32([keypoints_2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
        "\n",
        "# find the homography matrix\n",
        "transformation_matrix, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransacReprojThreshold=10)\n",
        "\n",
        "# log\n",
        "print(f\"Transformation:\\n{transformation_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# merging two images based on best matched key-points\n",
        "height1, width1 = scene_1.shape[:2]\n",
        "height2, width2 = scene_2.shape[:2]\n",
        "\n",
        "corners1 = np.float32([[0, 0], [0, height1], [width1, height1], [width1, 0]]).reshape(-1, 1, 2)\n",
        "corners2 = np.float32([[0, 0], [0, height2], [width2, height2], [width2, 0]]).reshape(-1, 1, 2)\n",
        "\n",
        "corners2_transformed = cv2.perspectiveTransform(corners2, transformation_matrix)\n",
        "all_corners = np.concatenate((corners1, corners2_transformed), axis=0)\n",
        "\n",
        "[x_min, y_min] = np.int32(all_corners.min(axis=0).ravel() - 0.5)\n",
        "[x_max, y_max] = np.int32(all_corners.max(axis=0).ravel() + 0.5)\n",
        "\n",
        "translation_matrix = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n",
        "\n",
        "panorama = cv2.warpPerspective(scene_1, translation_matrix.dot(transformation_matrix), (x_max - x_min, y_max - y_min))\n",
        "panorama[-y_min:height1 - y_min, -x_min:width1 - x_min] = scene_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n",
        "gs = fig.add_gridspec(2, 2)\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "ax1.imshow(cv2.cvtColor(scene_1, cv2.COLOR_BGR2RGB))\n",
        "ax1.set_title('Scene 1')\n",
        "ax1.axis('off')\n",
        "ax2 = fig.add_subplot(gs[0, 1])\n",
        "ax2.imshow(cv2.cvtColor(scene_2, cv2.COLOR_BGR2RGB))\n",
        "ax2.set_title('Scene 2')\n",
        "ax2.axis('off')\n",
        "ax3 = fig.add_subplot(gs[1, :])\n",
        "ax3.imshow(cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB))\n",
        "ax3.set_title('Merged')\n",
        "ax3.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "author_name": "Amirhossein Heydari",
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}