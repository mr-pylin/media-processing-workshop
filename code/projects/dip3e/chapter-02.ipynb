{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4;\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/media-processing-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"display: flex; justify-content: flex-end; flex: 1; gap: 8px; align-items: center; padding: 0;\">\n",
    "        <a href=\"https://opencv.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../../assets/images/libraries/opencv/logo/OpenCV_logo_no_text-1.svg\"\n",
    "                 alt=\"OpenCV Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://pillow.readthedocs.io/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../../assets/images/libraries/pillow/logo/pillow-logo-248x250.png\"\n",
    "                 alt=\"PIL Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scikit-image.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../../assets/images/libraries/scikit-image/logo/logo.png\"\n",
    "                 alt=\"scikit-image Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scipy.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../../assets/images/libraries/scipy/logo/logo.svg\"\n",
    "                 alt=\"SciPy Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Loading Images](#toc2_)    \n",
    "- [Projects](#toc3_)    \n",
    "  - [Exercise 1](#toc3_1_)    \n",
    "    - [(a)](#toc3_1_1_)    \n",
    "    - [(b)](#toc3_1_2_)    \n",
    "    - [(c)](#toc3_1_3_)    \n",
    "  - [Exercise 2](#toc3_2_)    \n",
    "    - [(a)](#toc3_2_1_)    \n",
    "    - [(b)](#toc3_2_2_)    \n",
    "  - [Exercise 3](#toc3_3_)    \n",
    "    - [(a)](#toc3_3_1_)    \n",
    "    - [(b)](#toc3_3_2_)    \n",
    "    - [(c)](#toc3_3_3_)    \n",
    "  - [Exercise 4](#toc3_4_)    \n",
    "    - [(a)](#toc3_4_1_)    \n",
    "    - [(b)](#toc3_4_2_)    \n",
    "    - [(c)](#toc3_4_3_)    \n",
    "  - [Exercise 5](#toc3_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Circle, Rectangle\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable automatic figure display (plt.show() required)  \n",
    "# this ensures consistency with .py scripts and gives full control over when plots appear\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Loading Images](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1 = cv2.imread(\"../../../assets/images/dip_3rd/CH02_Fig0222(b)(cameraman).tif\", cv2.IMREAD_GRAYSCALE)\n",
    "im_2 = cv2.imread(\"../../../assets/images/dip_3rd/CH06_Fig0638(a)(lenna_RGB).tif\", cv2.IMREAD_COLOR_RGB)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, vmin=0, vmax=255, cmap=\"gray\")\n",
    "axs[0].set_title(\"CH02_Fig0222(b)(cameraman).tif\")\n",
    "axs[1].imshow(im_2, vmin=0, vmax=255)\n",
    "axs[1].set_title(\"CH06_Fig0638(a)(lenna_RGB).tif\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Projects](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Exercise 1](#toc0_)\n",
    "\n",
    "- This exercise is based on [**PROJECT 02-01**](https://www.imageprocessingplace.com/DIP-3E/dip3e_student_projects.htm#02-01) from the textbook's student projects page.\n",
    "- Follow the instructions there to implement the required image processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[(a)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 halftone patterns (3x3 blocks)\n",
    "halftone_patterns = [\n",
    "    np.array([[0, 0, 0], [0, 0, 0], [0, 0, 0]], dtype=np.uint8),  # black\n",
    "    np.array([[0, 1, 0], [0, 0, 0], [0, 0, 0]], dtype=np.uint8),\n",
    "    np.array([[0, 1, 0], [0, 0, 0], [0, 0, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 0], [0, 0, 0], [0, 0, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 0], [0, 0, 0], [1, 0, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 1], [0, 0, 0], [1, 0, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 1], [0, 0, 1], [1, 0, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 1], [0, 0, 1], [1, 1, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 1], [1, 0, 1], [1, 1, 1]], dtype=np.uint8),\n",
    "    np.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]], dtype=np.uint8),  # white\n",
    "]\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(2, 5, figsize=(10, 4), layout=\"compressed\")\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    pattern = halftone_patterns[i]\n",
    "    h, w = pattern.shape\n",
    "\n",
    "    # set background to white [to simulate paper]\n",
    "    ax.set_facecolor(\"white\")\n",
    "\n",
    "    # draw black dots [to simulate black inc]\n",
    "    for r in range(h):\n",
    "        for c in range(w):\n",
    "            if pattern[r, c] == 0:  # black dot\n",
    "                circ = Circle((c, r), radius=0.5, color=\"black\")\n",
    "                ax.add_patch(circ)\n",
    "\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    ax.set(title=f\"Pattern {i}\", xticks=(), yticks=(), xlim=(-0.5, w - 0.5), ylim=(h - 0.5, -0.5), aspect=\"equal\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_to_paper(\n",
    "    img: NDArray[np.uint8],\n",
    "    pattern_size: tuple[int, int] = halftone_patterns[0].shape,\n",
    "    max_width_in: float = 8.5,\n",
    "    max_height_in: float = 11.0,\n",
    "    dpi: int = 300,\n",
    ") -> NDArray[np.uint8]:\n",
    "    \"\"\"\n",
    "    Scale an image so it fits within a physical paper size at the given DPI.\n",
    "\n",
    "    Parameters:\n",
    "        img          : Input image (grayscale or color)\n",
    "        pattern_size : Tuple (pattern_height, pattern_width), size of halftone block in pixels\n",
    "        max_width_in : Maximum paper width in inches\n",
    "        max_height_in: Maximum paper height in inches\n",
    "        dpi          : Dots per inch (printer resolution)\n",
    "\n",
    "    Returns:\n",
    "        img_scaled   : Resized image that, after expanding each pixel to a halftone block, fits within the paper\n",
    "    \"\"\"\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "    pattern_h, pattern_w = pattern_size\n",
    "\n",
    "    # compute max pixel dimensions for the paper\n",
    "    max_w_px = int(max_width_in * dpi)\n",
    "    max_h_px = int(max_height_in * dpi)\n",
    "\n",
    "    # compute scaling factor (never upscale)\n",
    "    scale = min(max_w_px / (w * pattern_w), max_h_px / (h * pattern_h), 1.0)\n",
    "\n",
    "    if scale < 1.0:\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        img_scaled = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    else:\n",
    "        img_scaled = img.copy()\n",
    "\n",
    "    return img_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def halftone(img: NDArray[np.uint8], halftone_patterns: list[NDArray]) -> NDArray[np.uint8]:\n",
    "    \"\"\"\n",
    "    Convert a grayscale or RGB image into a halftone image using provided dot patterns.\n",
    "\n",
    "    Each pixel in the input image is replaced by a halftone block corresponding to its\n",
    "    quantized intensity. For RGB images, each channel is processed independently.\n",
    "\n",
    "    Parameters:\n",
    "        img               : Input image, either grayscale (HxW) or RGB (HxWx3)\n",
    "        halftone_patterns : List of 2D arrays representing halftone blocks for each intensity level.\n",
    "                            All blocks must have the same shape (h_block, w_block).\n",
    "\n",
    "    Returns:\n",
    "        Output image with halftone applied. Shape is\n",
    "        (H*h_block, W*w_block) for grayscale or\n",
    "        (H*h_block, W*w_block, 3) for RGB.\n",
    "        Pixel values are scaled to 0‚Äì255.\n",
    "    \"\"\"\n",
    "\n",
    "    # determine if the image is grayscale or RGB\n",
    "    if len(img.shape) == 2:\n",
    "        channels = [img]\n",
    "    elif len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        channels = [img[:, :, ch] for ch in range(3)]\n",
    "    else:\n",
    "        raise ValueError(\"Input image must be grayscale or RGB\")\n",
    "\n",
    "    halftoned_channels = []\n",
    "    quant_levels = len(halftone_patterns)\n",
    "\n",
    "    for channel in channels:\n",
    "\n",
    "        # quantize input intensities\n",
    "        im_quant = np.floor(channel / 255 * (quant_levels - 1)).astype(np.uint8)\n",
    "\n",
    "        # Initialize output for this channel\n",
    "        h, w = channel.shape\n",
    "        h_block, w_block = halftone_patterns[0].shape\n",
    "        img_out = np.zeros((h * h_block, w * w_block), dtype=np.uint8)\n",
    "\n",
    "        # Fill output with patterns\n",
    "        for r in range(h):\n",
    "            for c in range(w):\n",
    "                row_slice = slice(r * h_block, (r + 1) * h_block)\n",
    "                col_slice = slice(c * w_block, (c + 1) * w_block)\n",
    "                img_out[row_slice, col_slice] = halftone_patterns[im_quant[r, c]]\n",
    "\n",
    "        halftoned_channels.append(img_out * 255)\n",
    "\n",
    "    # stack channels if RGB, otherwise return grayscale\n",
    "    if len(halftoned_channels) == 1:\n",
    "        return halftoned_channels[0]\n",
    "    else:\n",
    "        return np.stack(halftoned_channels, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[(b)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and halftone\n",
    "pattern_h, pattern_w = halftone_patterns[0].shape\n",
    "wedge = np.tile(np.arange(256, dtype=np.uint8), (256, 1))\n",
    "wedge_scaled = scale_to_paper(wedge, (pattern_h, pattern_w), 8.5, 11, 300)\n",
    "wedge_halftone = halftone(wedge_scaled, halftone_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define zoom area in original image\n",
    "wedge_h_zoom = slice(0, 20)\n",
    "wedge_w_zoom = slice(100, 180)\n",
    "\n",
    "# compute corresponding zoom area in halftone image\n",
    "wedge_ht_h_zoom = slice(wedge_h_zoom.start * pattern_h, wedge_h_zoom.stop * pattern_h)\n",
    "wedge_ht_w_zoom = slice(wedge_w_zoom.start * pattern_w, wedge_w_zoom.stop * pattern_w)\n",
    "\n",
    "# rectangle positions and sizes\n",
    "rect_orig = Rectangle(\n",
    "    (wedge_w_zoom.start, wedge_h_zoom.start),\n",
    "    wedge_w_zoom.stop - wedge_w_zoom.start,\n",
    "    wedge_h_zoom.stop - wedge_h_zoom.start,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"red\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "rect_ht = Rectangle(\n",
    "    (wedge_ht_w_zoom.start, wedge_ht_h_zoom.start),\n",
    "    wedge_ht_w_zoom.stop - wedge_ht_w_zoom.start,\n",
    "    wedge_ht_h_zoom.stop - wedge_ht_h_zoom.start,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"red\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "gs = GridSpec(2, 8, figure=fig)\n",
    "ax_orig = fig.add_subplot(gs[:, :2])\n",
    "ax_orig.imshow(wedge, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax_orig.add_patch(rect_orig)\n",
    "ax_orig.set_title(\"Original\")\n",
    "ax_halftone = fig.add_subplot(gs[:, 2:4])\n",
    "ax_halftone.imshow(wedge_halftone, cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax_halftone.add_patch(rect_ht)\n",
    "ax_halftone.set_title(\"Halftone\")\n",
    "ax_zoom_orig = fig.add_subplot(gs[0, 4:])\n",
    "ax_zoom_orig.imshow(wedge[wedge_h_zoom, wedge_w_zoom], cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax_zoom_orig.set_title(\"Zoom Original\")\n",
    "ax_zoom_orig.axis(\"off\")\n",
    "ax_zoom_ht = fig.add_subplot(gs[1, 4:])\n",
    "ax_zoom_ht.imshow(wedge_halftone[wedge_ht_h_zoom, wedge_ht_w_zoom], cmap=\"gray\", vmin=0, vmax=255)\n",
    "ax_zoom_ht.set_title(\"Zoom Halftone\")\n",
    "ax_zoom_ht.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_3_'></a>[(c)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and halftone\n",
    "pattern_h, pattern_w = halftone_patterns[0].shape\n",
    "im1_scaled = scale_to_paper(im_1, (pattern_h, pattern_w), 8.5, 11, 300)\n",
    "im1_halftone = halftone(im1_scaled, halftone_patterns)\n",
    "\n",
    "# define zoom area in original image\n",
    "im1_h_zoom = slice(45, 75)\n",
    "im1_w_zoom = slice(105, 135)\n",
    "\n",
    "# compute corresponding zoom area in halftone image\n",
    "im1_ht_h_zoom = slice(im1_h_zoom.start * pattern_h, im1_h_zoom.stop * pattern_h)\n",
    "im1_ht_w_zoom = slice(im1_w_zoom.start * pattern_w, im1_w_zoom.stop * pattern_w)\n",
    "\n",
    "# rectangle positions and sizes\n",
    "rect_orig = Rectangle(\n",
    "    (im1_w_zoom.start, im1_h_zoom.start),\n",
    "    im1_w_zoom.stop - im1_w_zoom.start,\n",
    "    im1_h_zoom.stop - im1_h_zoom.start,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"red\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "rect_ht = Rectangle(\n",
    "    (im1_ht_w_zoom.start, im1_ht_h_zoom.start),\n",
    "    im1_ht_w_zoom.stop - im1_ht_w_zoom.start,\n",
    "    im1_ht_h_zoom.stop - im1_ht_h_zoom.start,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"red\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].add_patch(rect_orig)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im1_halftone, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].add_patch(rect_ht)\n",
    "axs[1].set_title(\"Halftone\")\n",
    "axs[2].imshow(im_1[im1_h_zoom, im1_w_zoom], cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"Zoom Original\")\n",
    "axs[3].imshow(im1_halftone[im1_ht_h_zoom, im1_ht_w_zoom], cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"Zoom Halftone\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and halftone\n",
    "pattern_h, pattern_w = halftone_patterns[0].shape\n",
    "im2_scaled = scale_to_paper(im_2, (pattern_h, pattern_w), 8.5, 11, 300)\n",
    "im2_halftone = halftone(im2_scaled, halftone_patterns)\n",
    "\n",
    "# define zoom area in original image\n",
    "im2_h_zoom = slice(255, 285)\n",
    "im2_w_zoom = slice(245, 275)\n",
    "\n",
    "# compute corresponding zoom area in halftone image\n",
    "im2_ht_h_zoom = slice(im2_h_zoom.start * pattern_h, im2_h_zoom.stop * pattern_h)\n",
    "im2_ht_w_zoom = slice(im2_w_zoom.start * pattern_w, im2_w_zoom.stop * pattern_w)\n",
    "\n",
    "# rectangle positions and sizes\n",
    "rect_orig = Rectangle(\n",
    "    (im2_w_zoom.start, im2_h_zoom.start),\n",
    "    im2_w_zoom.stop - im2_w_zoom.start,\n",
    "    im2_h_zoom.stop - im2_h_zoom.start,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"white\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "rect_ht = Rectangle(\n",
    "    (im2_ht_w_zoom.start, im2_ht_h_zoom.start),\n",
    "    im2_ht_w_zoom.stop - im2_ht_w_zoom.start,\n",
    "    im2_ht_h_zoom.stop - im2_ht_h_zoom.start,\n",
    "    linewidth=2,\n",
    "    edgecolor=\"white\",\n",
    "    facecolor=\"none\",\n",
    ")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].add_patch(rect_orig)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im2_halftone, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].add_patch(rect_ht)\n",
    "axs[1].set_title(\"Halftone\")\n",
    "axs[2].imshow(im_2[im2_h_zoom, im2_w_zoom], cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"Zoom Original\")\n",
    "axs[3].imshow(im2_halftone[im2_ht_h_zoom, im2_ht_w_zoom], cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"Zoom Halftone\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Exercise 2](#toc0_)\n",
    "\n",
    "- This exercise is based on [**PROJECT 02-02**](https://www.imageprocessingplace.com/DIP-3E/dip3e_student_projects.htm#02-02) from the textbook's student projects page.\n",
    "- Follow the instructions there to implement the required image processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[(a)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_intensity_levels(img: NDArray[np.uint8], target_levels: int) -> NDArray[np.uint8]:\n",
    "    \"\"\"\n",
    "    Reduce the number of intensity levels in an image.\n",
    "\n",
    "    Parameters:\n",
    "        img           : input image (grayscale)\n",
    "        target_levels : desired number of intensity levels (must be power of 2)\n",
    "\n",
    "    Returns:\n",
    "        img_quantized : processed image with reduced intensity levels\n",
    "    \"\"\"\n",
    "\n",
    "    # validate that target_levels is a power of 2\n",
    "    if target_levels <= 0 or (target_levels & (target_levels - 1)) != 0:\n",
    "        raise ValueError(\"Target levels must be a positive power of 2\")\n",
    "\n",
    "    # validate that target_levels is less than or equal to 256\n",
    "    if target_levels > 256:\n",
    "        raise ValueError(\"Target levels must be less than or equal to 256\")\n",
    "\n",
    "    # quantize the image\n",
    "    img_quantized = np.round((img / 255) * (target_levels - 1)) / (target_levels - 1) * 255\n",
    "    img_quantized = np.clip(img_quantized, 0, 255).astype(np.uint8)\n",
    "\n",
    "    return img_quantized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[(b)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_scale_1 = reduce_intensity_levels(im_1, target_levels=128)\n",
    "im1_scale_2 = reduce_intensity_levels(im_1, target_levels=32)\n",
    "im1_scale_3 = reduce_intensity_levels(im_1, target_levels=8)\n",
    "im1_scale_4 = reduce_intensity_levels(im_1, target_levels=2)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im1_scale_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"levels=128\")\n",
    "axs[2].imshow(im1_scale_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"levels=32\")\n",
    "axs[3].imshow(im1_scale_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"levels=8\")\n",
    "axs[4].imshow(im1_scale_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"levels=2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2_scale_1 = reduce_intensity_levels(im_2, target_levels=128)\n",
    "im2_scale_2 = reduce_intensity_levels(im_2, target_levels=32)\n",
    "im2_scale_3 = reduce_intensity_levels(im_2, target_levels=8)\n",
    "im2_scale_4 = reduce_intensity_levels(im_2, target_levels=2)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im2_scale_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"levels=128\")\n",
    "axs[2].imshow(im2_scale_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"levels=32\")\n",
    "axs[3].imshow(im2_scale_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"levels=8\")\n",
    "axs[4].imshow(im2_scale_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"levels=2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Exercise 3](#toc0_)\n",
    "\n",
    "- This exercise is based on [**PROJECT 02-03**](https://www.imageprocessingplace.com/DIP-3E/dip3e_student_projects.htm#02-03) from the textbook's student projects page.\n",
    "- Follow the instructions there to implement the required image processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_1_'></a>[(a)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_shrink_pixel_replication(img: NDArray[np.uint8], factor: int, mode: str = \"zoom\") -> NDArray[np.uint8]:\n",
    "    \"\"\"\n",
    "    Zoom or shrink an image using pixel replication.\n",
    "\n",
    "    Parameters:\n",
    "        img    : input image (grayscale or RGB)\n",
    "        factor : integer zoom/shrink factor (>0)\n",
    "        mode   : 'zoom' to enlarge, 'shrink' to reduce size\n",
    "\n",
    "    Returns:\n",
    "        Processed image\n",
    "    \"\"\"\n",
    "\n",
    "    if factor <= 0 or not isinstance(factor, int):\n",
    "        raise ValueError(\"Factor must be a positive integer\")\n",
    "\n",
    "    if mode not in [\"zoom\", \"shrink\"]:\n",
    "        raise ValueError(\"Mode must be 'zoom' or 'shrink'\")\n",
    "\n",
    "    if mode == \"zoom\":\n",
    "        return np.repeat(np.repeat(img, factor, axis=0), factor, axis=1)\n",
    "\n",
    "    elif mode == \"shrink\":\n",
    "        return img[::factor, ::factor] if img.ndim == 2 else img[::factor, ::factor, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_2_'></a>[(b)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_shrink_1 = zoom_shrink_pixel_replication(im_1, factor=2, mode=\"shrink\")\n",
    "im1_shrink_2 = zoom_shrink_pixel_replication(im_1, factor=4, mode=\"shrink\")\n",
    "im1_shrink_3 = zoom_shrink_pixel_replication(im_1, factor=8, mode=\"shrink\")\n",
    "im1_shrink_4 = zoom_shrink_pixel_replication(im_1, factor=16, mode=\"shrink\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im1_shrink_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im1_shrink_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im1_shrink_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im1_shrink_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2_shrink_1 = zoom_shrink_pixel_replication(im_2, factor=2, mode=\"shrink\")\n",
    "im2_shrink_2 = zoom_shrink_pixel_replication(im_2, factor=4, mode=\"shrink\")\n",
    "im2_shrink_3 = zoom_shrink_pixel_replication(im_2, factor=8, mode=\"shrink\")\n",
    "im2_shrink_4 = zoom_shrink_pixel_replication(im_2, factor=16, mode=\"shrink\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im2_shrink_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im2_shrink_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im2_shrink_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im2_shrink_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_3_'></a>[(c)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_zoom_1 = zoom_shrink_pixel_replication(im1_shrink_1, factor=2, mode=\"zoom\")\n",
    "im1_zoom_2 = zoom_shrink_pixel_replication(im1_shrink_2, factor=4, mode=\"zoom\")\n",
    "im1_zoom_3 = zoom_shrink_pixel_replication(im1_shrink_3, factor=8, mode=\"zoom\")\n",
    "im1_zoom_4 = zoom_shrink_pixel_replication(im1_shrink_4, factor=16, mode=\"zoom\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im1_zoom_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im1_zoom_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im1_zoom_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im1_zoom_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2_zoom_1 = zoom_shrink_pixel_replication(im2_shrink_1, factor=2, mode=\"zoom\")\n",
    "im2_zoom_2 = zoom_shrink_pixel_replication(im2_shrink_2, factor=4, mode=\"zoom\")\n",
    "im2_zoom_3 = zoom_shrink_pixel_replication(im2_shrink_3, factor=8, mode=\"zoom\")\n",
    "im2_zoom_4 = zoom_shrink_pixel_replication(im2_shrink_4, factor=16, mode=\"zoom\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im2_zoom_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im2_zoom_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im2_zoom_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im2_zoom_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Exercise 4](#toc0_)\n",
    "\n",
    "- This exercise is based on [**PROJECT 02-04**](https://www.imageprocessingplace.com/DIP-3E/dip3e_student_projects.htm#02-04) from the textbook's student projects page.\n",
    "- Follow the instructions there to implement the required image processing tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_1_'></a>[(a)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilinear_resize(img: NDArray[np.uint8], input_dpi: float, target_dpi: float) -> NDArray[np.uint8]:\n",
    "    \"\"\"\n",
    "    Zoom or shrink an image using bilinear interpolation based on DPI.\n",
    "\n",
    "    Parameters:\n",
    "        img        : Input image (grayscale or RGB)\n",
    "        input_dpi  : DPI of the input image\n",
    "        target_dpi : Desired DPI of the output image\n",
    "\n",
    "    Returns:\n",
    "        Resized image\n",
    "    \"\"\"\n",
    "\n",
    "    if input_dpi <= 0 or target_dpi <= 0:\n",
    "        raise ValueError(\"DPI values must be positive\")\n",
    "\n",
    "    # compute scaling factor\n",
    "    scale = target_dpi / input_dpi\n",
    "    h_in, w_in = img.shape[:2]\n",
    "    h_out, w_out = int(h_in * scale), int(w_in * scale)\n",
    "\n",
    "    # generate coordinates in the output image\n",
    "    row_idx = (np.arange(h_out) + 0.5) / scale - 0.5\n",
    "    col_idx = (np.arange(w_out) + 0.5) / scale - 0.5\n",
    "\n",
    "    row_idx = np.clip(row_idx, 0, h_in - 1)\n",
    "    col_idx = np.clip(col_idx, 0, w_in - 1)\n",
    "\n",
    "    r_floor = np.floor(row_idx).astype(int)\n",
    "    r_ceil = np.ceil(row_idx).astype(int)\n",
    "    c_floor = np.floor(col_idx).astype(int)\n",
    "    c_ceil = np.ceil(col_idx).astype(int)\n",
    "\n",
    "    r_weight = row_idx - r_floor\n",
    "    c_weight = col_idx - c_floor\n",
    "\n",
    "    if img.ndim == 2:\n",
    "        out_img = np.zeros((h_out, w_out), dtype=np.float32)\n",
    "        for i, (rf, rcw) in enumerate(zip(r_floor, r_weight)):\n",
    "            for j, (cf, ccw) in enumerate(zip(c_floor, c_weight)):\n",
    "                top_left = img[rf, cf]\n",
    "                top_right = img[rf, c_ceil[j]]\n",
    "                bottom_left = img[r_ceil[i], cf]\n",
    "                bottom_right = img[r_ceil[i], c_ceil[j]]\n",
    "\n",
    "                top = top_left * (1 - ccw) + top_right * ccw\n",
    "                bottom = bottom_left * (1 - ccw) + bottom_right * ccw\n",
    "                out_img[i, j] = top * (1 - rcw) + bottom * rcw\n",
    "    else:\n",
    "        out_img = np.zeros((h_out, w_out, img.shape[2]), dtype=np.float32)\n",
    "        for i, (rf, rcw) in enumerate(zip(r_floor, r_weight)):\n",
    "            for j, (cf, ccw) in enumerate(zip(c_floor, c_weight)):\n",
    "                for k in range(img.shape[2]):\n",
    "                    top_left = img[rf, cf, k]\n",
    "                    top_right = img[rf, c_ceil[j], k]\n",
    "                    bottom_left = img[r_ceil[i], cf, k]\n",
    "                    bottom_right = img[r_ceil[i], c_ceil[j], k]\n",
    "\n",
    "                    top = top_left * (1 - ccw) + top_right * ccw\n",
    "                    bottom = bottom_left * (1 - ccw) + bottom_right * ccw\n",
    "                    out_img[i, j, k] = top * (1 - rcw) + bottom * rcw\n",
    "\n",
    "    return np.clip(out_img, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_2_'></a>[(b)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_shrink_1 = bilinear_resize(im_1, input_dpi=300, target_dpi=200)\n",
    "im1_shrink_2 = bilinear_resize(im_1, input_dpi=300, target_dpi=150)\n",
    "im1_shrink_3 = bilinear_resize(im_1, input_dpi=300, target_dpi=100)\n",
    "im1_shrink_4 = bilinear_resize(im_1, input_dpi=300, target_dpi=50)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im1_shrink_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im1_shrink_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im1_shrink_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im1_shrink_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2_shrink_1 = bilinear_resize(im_2, input_dpi=300, target_dpi=200)\n",
    "im2_shrink_2 = bilinear_resize(im_2, input_dpi=300, target_dpi=150)\n",
    "im2_shrink_3 = bilinear_resize(im_2, input_dpi=300, target_dpi=100)\n",
    "im2_shrink_4 = bilinear_resize(im_2, input_dpi=300, target_dpi=50)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im2_shrink_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im2_shrink_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im2_shrink_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im2_shrink_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_3_'></a>[(c)](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_zoom_1 = bilinear_resize(im1_shrink_1, input_dpi=200, target_dpi=300)\n",
    "im1_zoom_2 = bilinear_resize(im1_shrink_2, input_dpi=150, target_dpi=300)\n",
    "im1_zoom_3 = bilinear_resize(im1_shrink_3, input_dpi=100, target_dpi=300)\n",
    "im1_zoom_4 = bilinear_resize(im1_shrink_4, input_dpi=50, target_dpi=300)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im1_zoom_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im1_zoom_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im1_zoom_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im1_zoom_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im2_zoom_1 = bilinear_resize(im2_shrink_1, input_dpi=200, target_dpi=300)\n",
    "im2_zoom_2 = bilinear_resize(im2_shrink_2, input_dpi=150, target_dpi=300)\n",
    "im2_zoom_3 = bilinear_resize(im2_shrink_3, input_dpi=100, target_dpi=300)\n",
    "im2_zoom_4 = bilinear_resize(im2_shrink_4, input_dpi=50, target_dpi=300)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(im2_zoom_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"factor=2\")\n",
    "axs[2].imshow(im2_zoom_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"factor=4\")\n",
    "axs[3].imshow(im2_zoom_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"factor=8\")\n",
    "axs[4].imshow(im2_zoom_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"factor=16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Exercise 5](#toc0_)\n",
    "\n",
    "- This exercise is based on [**PROJECT 02-05**](https://www.imageprocessingplace.com/DIP-3E/dip3e_student_projects.htm#02-05) from the textbook's student projects page.\n",
    "- Follow the instructions there to implement the required image processing tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_arithmetic(\n",
    "    img1: NDArray[np.uint8],\n",
    "    img2: NDArray[np.uint8] | float | int,\n",
    "    operation: str,\n",
    ") -> NDArray[np.uint8]:\n",
    "    \"\"\"\n",
    "    Perform arithmetic operations between two images or an image and a scalar.\n",
    "\n",
    "    Parameters:\n",
    "        img1      : First input image (grayscale or RGB)\n",
    "        img2      : Second input image or scalar\n",
    "        operation : 'add', 'subtract', 'multiply', 'divide'\n",
    "\n",
    "    Returns:\n",
    "        Resulting image as uint8 with values clipped to [0, 255]\n",
    "    \"\"\"\n",
    "\n",
    "    if operation not in [\"add\", \"subtract\", \"multiply\", \"divide\"]:\n",
    "        raise ValueError(\"Operation must be 'add', 'subtract', 'multiply', or 'divide'\")\n",
    "\n",
    "    # convert inputs to float for safe arithmetic\n",
    "    img1_f = img1.astype(np.float32)\n",
    "\n",
    "    if isinstance(img2, (int, float)):\n",
    "        img2_f = float(img2)\n",
    "    else:\n",
    "        img2_f = img2.astype(np.float32)\n",
    "        if img1.shape != img2.shape:\n",
    "            raise ValueError(\"Image shapes must match for image-image operations\")\n",
    "\n",
    "    if operation == \"add\":\n",
    "        result = img1_f + img2_f\n",
    "\n",
    "    elif operation == \"subtract\":\n",
    "        result = img1_f - img2_f\n",
    "\n",
    "    elif operation == \"multiply\":\n",
    "        result = img1_f * img2_f\n",
    "\n",
    "    elif operation == \"divide\":\n",
    "        # avoid division by zero\n",
    "        if isinstance(img2_f, (int, float)):\n",
    "            if img2_f == 0:\n",
    "                raise ValueError(\"Cannot divide by zero scalar\")\n",
    "        else:\n",
    "            img2_f = np.where(img2_f == 0, 1e-6, img2_f)\n",
    "        result = img1_f / img2_f\n",
    "\n",
    "    # clip to [0, 255] and convert to uint8\n",
    "    result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = im_1.copy()\n",
    "im2 = im_2.copy()[::2, ::2].mean(axis=2)\n",
    "\n",
    "# perform arithmetic operations\n",
    "arithmetic_1 = image_arithmetic(im1, im2, operation=\"add\")\n",
    "arithmetic_2 = image_arithmetic(im1, im2, operation=\"subtract\")\n",
    "arithmetic_3 = image_arithmetic(im1, im2, operation=\"multiply\")\n",
    "arithmetic_4 = image_arithmetic(im1, 2, operation=\"multiply\")\n",
    "arithmetic_5 = image_arithmetic(im1, im2, operation=\"divide\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5), layout=\"compressed\")\n",
    "axs[0].imshow(arithmetic_1, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[0].set_title(\"arithmetic_1\")\n",
    "axs[1].imshow(arithmetic_2, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[1].set_title(\"arithmetic_2\")\n",
    "axs[2].imshow(arithmetic_3, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[2].set_title(\"arithmetic_3\")\n",
    "axs[3].imshow(arithmetic_4, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[3].set_title(\"arithmetic_4\")\n",
    "axs[4].imshow(arithmetic_5, cmap=\"gray\", vmin=0, vmax=255)\n",
    "axs[4].set_title(\"arithmetic_5\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "media-processing-workshop (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
