{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/media-processing-workshop](https://github.com/mr-pylin/media-processing-workshop)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Dependencies](#toc1_)    \n",
        "- [Load Images](#toc2_)    \n",
        "- [Entropy](#toc3_)    \n",
        "- [Redundancy](#toc4_)    \n",
        "  - [Coding Redundancy](#toc4_1_)    \n",
        "    - [Huffman Coding](#toc4_1_1_)    \n",
        "    - [Arithmetic Coding](#toc4_1_2_)    \n",
        "    - [Lempel-Ziv-Welch (LZW) Coding](#toc4_1_3_)    \n",
        "  - [Interpixel (Spatial) Redundancy](#toc4_2_)    \n",
        "    - [Run-Length Encoding (RLE)](#toc4_2_1_)    \n",
        "    - [Differential Encoding](#toc4_2_2_)    \n",
        "    - [Predictive Coding](#toc4_2_3_)    \n",
        "    - [Transform Coding](#toc4_2_4_)    \n",
        "    - [Wavelet Transform](#toc4_2_5_)    \n",
        "    - [Block-based Compression](#toc4_2_6_)    \n",
        "  - [Temporal Redundancy](#toc4_3_)    \n",
        "    - [Inter-frame Compression (Predictive Compression)](#toc4_3_1_)    \n",
        "    - [Motion Compensation](#toc4_3_2_)    \n",
        "    - [Differential Encoding](#toc4_3_3_)    \n",
        "    - [Keyframe Extraction](#toc4_3_4_)    \n",
        "    - [Temporal Filtering](#toc4_3_5_)    \n",
        "    - [Block-based Motion Estimation](#toc4_3_6_)    \n",
        "    - [Long-Term Prediction](#toc4_3_7_)    \n",
        "  - [Psychovisual Redundancy](#toc4_4_)    \n",
        "    - [Chrominance Subsampling](#toc4_4_1_)    \n",
        "    - [Quantization](#toc4_4_2_)    \n",
        "    - [Perceptual Coding](#toc4_4_3_)    \n",
        "    - [Rate-Distortion Optimization](#toc4_4_4_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Dependencies](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import heapq\n",
        "from collections import Counter\n",
        "from decimal import Decimal, getcontext\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Load Images](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_1 = cv2.imread(\"../../assets/images/dip_3rd/CH02_Fig0222(b)(cameraman).tif\", cv2.IMREAD_GRAYSCALE)\n",
        "im_2 = cv2.cvtColor(cv2.imread(\"../../assets/images/dip_3rd/CH06_Fig0638(a)(lenna_RGB).tif\"), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), layout=\"compressed\")\n",
        "axs[0].imshow(im_1, vmin=0, vmax=255, cmap=\"gray\")\n",
        "axs[0].set_title(\"CH02_Fig0222(b)(cameraman).tif\")\n",
        "axs[1].imshow(im_2, vmin=0, vmax=255)\n",
        "axs[1].set_title(\"CH06_Fig0638(a)(lenna_RGB).tif\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_3 = cv2.imread(\"../../assets/images/original/raster/entropy/entropy_4c.png\", flags=cv2.IMREAD_GRAYSCALE)\n",
        "im_4 = cv2.imread(\"../../assets/images/original/raster/entropy/entropy_32c_1.png\", flags=cv2.IMREAD_GRAYSCALE)\n",
        "im_5 = cv2.imread(\"../../assets/images/original/raster/entropy/entropy_32c_2.png\", flags=cv2.IMREAD_GRAYSCALE)\n",
        "im_6 = cv2.imread(\"../../assets/images/original/raster/entropy/entropy_32c_3.png\", flags=cv2.IMREAD_GRAYSCALE)\n",
        "im_7 = cv2.imread(\"../../assets/images/original/raster/entropy/entropy_256c.png\", flags=cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# plot\n",
        "images = [im_3, im_4, im_5, im_6, im_7]\n",
        "titles = [\n",
        "    f\"{len(np.unique(im_3))} colors\",\n",
        "    f\"{len(np.unique(im_4))} colors\",\n",
        "    f\"{len(np.unique(im_5))} colors\",\n",
        "    f\"{len(np.unique(im_6))} colors\",\n",
        "    f\"{len(np.unique(im_7))} colors\",\n",
        "]\n",
        "fig, axs = plt.subplots(nrows=2, ncols=len(images), figsize=(4 * len(images), 8), layout=\"compressed\")\n",
        "for i in range(len(images)):\n",
        "    axs[0, i].imshow(images[i], cmap=\"gray\", vmin=0, vmax=255)\n",
        "    axs[0, i].set(title=titles[i], xticks=[], yticks=[])\n",
        "    axs[1, i].hist(images[i].flatten(), bins=256, range=(0, 256))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[Entropy](#toc0_)\n",
        "- Entropy is a fundamental concept in information theory, **measuring the average amount of information** in a signal.\n",
        "- It represents the **minimum number of bits** required to encode a signal **without loss**.\n",
        "- The entropy $H$ of an image is given by Shannon's formula:\n",
        "  $$H = - \\sum_{i=1}^{N} P(x_i) \\log_2 P(x_i)$$\n",
        "- **Interpretation:**\n",
        "  - **Low entropy (<1 bpp)** ‚Üí The image has redundant information (e.g., large uniform areas).\n",
        "  - **High entropy (~8 bpp for grayscale images)** ‚Üí The image is highly detailed with little redundancy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_entropy(image: np.ndarray) -> float:\n",
        "    image_flat = image.flatten()\n",
        "    _, counts = np.unique(image_flat, return_counts=True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
        "    return entropy.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# average number of bits needed to encode each pixel in the image optimally\n",
        "print(f\"H(im_3): {calculate_entropy(im_3)}\")\n",
        "print(f\"H(im_4): {calculate_entropy(im_4)}\")\n",
        "print(f\"H(im_5): {calculate_entropy(im_5)}\")\n",
        "print(f\"H(im_6): {calculate_entropy(im_6)}\")\n",
        "print(f\"H(im_7): {calculate_entropy(im_7)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc4_'></a>[Redundancy](#toc0_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_1_'></a>[Coding Redundancy](#toc0_)\n",
        "\n",
        "- This occurs when an **image‚Äôs pixel values** are encoded using **more bits than necessary**.\n",
        "- If a grayscale image uses **8 bits per pixel (bpp)** but the intensity values only range from **0 to 100**, we are wasting storage by not optimizing the encoding.\n",
        "- By optimizing encoding, we can reduce image size without losing any information (**lossless compression**).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_6_flatten = im_6.flatten().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_1_1_'></a>[Huffman Coding](#toc0_)\n",
        "\n",
        "- It is an **entropy-based, variable-length** coding scheme that minimizes the average number of bits used per symbol by exploiting the **non-uniform** distribution of pixel values.\n",
        "- The objective is to assign **shorter codewords to more frequent symbols** and **longer codewords to less frequent ones**.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [**A Method for the Construction of Minimum-Redundancy Codes**](https://ieeexplore.ieee.org/abstract/document/4051119/) by [David A. Huffman](https://ieeexplore.ieee.org/author/37338941400) in *1952*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _HuffmanNode:\n",
        "    def __init__(self, value=None, freq=0):\n",
        "        self.value = value\n",
        "        self.freq = freq\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "    def __lt__(self, other):\n",
        "        return self.freq < other.freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class HuffmanCoding:\n",
        "    def __init__(self):\n",
        "        self._codes = {}\n",
        "        self._reverse_codes = {}\n",
        "        self._tree_built = False\n",
        "\n",
        "    def encode(self, data: list) -> str:\n",
        "        root = self._build_tree(data)\n",
        "        self._build_codes(root)\n",
        "        self._tree_built = True\n",
        "        return \"\".join(self._codes[item] for item in data)\n",
        "\n",
        "    def decode(self, encoded_data: str) -> list:\n",
        "        if not self._tree_built:\n",
        "            raise ValueError(\"Huffman tree has not been built. Call `encode` first.\")\n",
        "        current_code = \"\"\n",
        "        decoded_data = []\n",
        "\n",
        "        for bit in encoded_data:\n",
        "            current_code += bit\n",
        "            if current_code in self._reverse_codes:\n",
        "                decoded_data.append(self._reverse_codes[current_code])\n",
        "                current_code = \"\"\n",
        "\n",
        "        return decoded_data\n",
        "\n",
        "    def get_codebook(self) -> dict:\n",
        "        if not self._tree_built:\n",
        "            raise ValueError(\"Codebook is not available. Call `encode` first.\")\n",
        "        return self._codes.copy()\n",
        "\n",
        "    def _build_tree(self, data: list) -> _HuffmanNode:\n",
        "        frequency = Counter(data)\n",
        "        heap = [_HuffmanNode(value, freq) for value, freq in frequency.items()]\n",
        "        heapq.heapify(heap)\n",
        "\n",
        "        while len(heap) > 1:\n",
        "            node1 = heapq.heappop(heap)\n",
        "            node2 = heapq.heappop(heap)\n",
        "            merged = _HuffmanNode(freq=node1.freq + node2.freq)\n",
        "            merged.left = node1\n",
        "            merged.right = node2\n",
        "            heapq.heappush(heap, merged)\n",
        "\n",
        "        return heap[0] if heap else None\n",
        "\n",
        "    def _build_codes(self, node: _HuffmanNode, current_code: str = \"\"):\n",
        "        if node is None:\n",
        "            return\n",
        "\n",
        "        if node.value is not None:\n",
        "            self._codes[node.value] = current_code\n",
        "            self._reverse_codes[current_code] = node.value\n",
        "            return\n",
        "\n",
        "        self._build_codes(node.left, current_code + \"0\")\n",
        "        self._build_codes(node.right, current_code + \"1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_codebook_size(codebook: dict[int, str]):\n",
        "    keys = len(codebook) * np.log2(np.iinfo(im_6.dtype).max + 1)\n",
        "    values = sum(map(lambda x: len(x), codebook.values()))\n",
        "    return int(keys + values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "huffman_coding = HuffmanCoding()\n",
        "huffman_enc = huffman_coding.encode(im_6_flatten)\n",
        "huffman_dec = huffman_coding.decode(huffman_enc)\n",
        "codebook = huffman_coding.get_codebook()\n",
        "im_6_dec = np.array(huffman_dec, dtype=np.uint8).reshape(im_6.shape)\n",
        "is_equal = np.array_equal(im_6, im_6_dec)\n",
        "\n",
        "# log\n",
        "print(f\"im_6        (bitstream length)      : {int(np.prod(im_6.shape) * np.log2(np.iinfo(im_6.dtype).max + 1))} bits\")\n",
        "print(f\"huffman_enc (bitstream length)      : {len(huffman_enc)} + {calculate_codebook_size(codebook)} bits\")\n",
        "print(f\"entropy     (avg. bitstream length) : {calculate_entropy(im_6) * np.prod(im_6.shape)} bits\")\n",
        "print(f\"is_equal    (lossless)              : {is_equal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# huffman codebook\n",
        "print(\"\\nhuffman codebook:\")\n",
        "for symbol, code in huffman_coding.get_codebook().items():\n",
        "    print(f\"symbol: {symbol:3} -> code: {code}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_1_2_'></a>[Arithmetic Coding](#toc0_)\n",
        "\n",
        "- Arithmetic coding is a more **advanced** form of **entropy coding** rather than **Huffman coding**.\n",
        "- It encodes the entire message as a **fraction (or interval)** within a range between **0 and 1**, based on the **cumulative probabilities** of the symbols in the message.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [**A universal algorithm for sequential data compression**](https://ieeexplore.ieee.org/abstract/document/1055714) by [Jacob Ziv\n",
        "](https://ieeexplore.ieee.org/author/37267355900) in *1977*.\n",
        "- This paper introduced the core idea of arithmetic coding.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# base code: https://github.com/ahmedfgad/ArithmeticEncodingPython/blob/main/pyae.py\n",
        "\n",
        "getcontext().prec = 288\n",
        "\n",
        "\n",
        "class ArithmeticEncoding:\n",
        "    \"\"\"\n",
        "    ArithmeticEncoding is a class for building the arithmetic encoding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, save_stages=False):\n",
        "        \"\"\"\n",
        "        save_stages: If True, then the intervals of each stage are saved in a list.\n",
        "                      Note that setting save_stages=True may cause memory overflow if the message is large.\n",
        "        \"\"\"\n",
        "        self.save_stages = save_stages\n",
        "        if save_stages:\n",
        "            print(\"WARNING: Setting save_stages=True may cause memory overflow if the message is large.\")\n",
        "\n",
        "    def get_probability_table(self, frequency_table):\n",
        "        \"\"\"\n",
        "        Calculates the probability table from the frequency table.\n",
        "        \"\"\"\n",
        "        total_frequency = sum(frequency_table.values())\n",
        "        return {key: Decimal(value) / total_frequency for key, value in frequency_table.items()}\n",
        "\n",
        "    def get_encoded_value(self, last_stage_probs):\n",
        "        \"\"\"\n",
        "        After encoding the entire message, this method returns the single value that represents the entire message.\n",
        "        \"\"\"\n",
        "        last_stage_values = [value for sublist in last_stage_probs.values() for value in sublist]\n",
        "        encoded_value = (min(last_stage_values) + max(last_stage_values)) / 2\n",
        "        return min(last_stage_values), max(last_stage_values), encoded_value\n",
        "\n",
        "    def process_stage(self, probability_table, stage_min, stage_max):\n",
        "        \"\"\"\n",
        "        Processing a stage in the encoding/decoding process.\n",
        "        \"\"\"\n",
        "        stage_domain = stage_max - stage_min\n",
        "        stage_probs = {}\n",
        "        cumulative_min = stage_min\n",
        "        for term, prob in probability_table.items():\n",
        "            cumulative_max = cumulative_min + prob * stage_domain\n",
        "            stage_probs[term] = [cumulative_min, cumulative_max]\n",
        "            cumulative_min = cumulative_max\n",
        "        return stage_probs\n",
        "\n",
        "    def encode(self, msg):\n",
        "        \"\"\"\n",
        "        Encodes a message using arithmetic encoding, calculating the frequency table internally.\n",
        "        \"\"\"\n",
        "        # calculate frequency table from message\n",
        "        frequency_table = dict(Counter(msg))\n",
        "\n",
        "        # get the probability table\n",
        "        probability_table = self.get_probability_table(frequency_table)\n",
        "\n",
        "        encoder = []\n",
        "        stage_min, stage_max = Decimal(0.0), Decimal(1.0)\n",
        "\n",
        "        for msg_term in msg:\n",
        "            stage_probs = self.process_stage(probability_table, stage_min, stage_max)\n",
        "            stage_min, stage_max = stage_probs[msg_term]\n",
        "            if self.save_stages:\n",
        "                encoder.append(stage_probs)\n",
        "\n",
        "        last_stage_probs = self.process_stage(probability_table, stage_min, stage_max)\n",
        "        if self.save_stages:\n",
        "            encoder.append(last_stage_probs)\n",
        "\n",
        "        interval_min_value, interval_max_value, encoded_msg = self.get_encoded_value(last_stage_probs)\n",
        "        return encoded_msg, encoder, interval_min_value, interval_max_value\n",
        "\n",
        "    def decode(self, encoded_msg, msg_length, probability_table):\n",
        "        \"\"\"\n",
        "        Decodes a message from a floating-point number, returning a list of characters.\n",
        "        \"\"\"\n",
        "        decoded_msg = []\n",
        "        stage_min, stage_max = Decimal(0.0), Decimal(1.0)\n",
        "\n",
        "        for _ in range(msg_length):\n",
        "            stage_probs = self.process_stage(probability_table, stage_min, stage_max)\n",
        "\n",
        "            for msg_term, (min_val, max_val) in stage_probs.items():\n",
        "                if min_val <= encoded_msg < max_val:\n",
        "                    decoded_msg.append(msg_term)\n",
        "                    stage_min, stage_max = min_val, max_val\n",
        "                    break\n",
        "\n",
        "            # optionally save the decoding stages (if needed, otherwise, you can remove this part)\n",
        "            # if you want to save decoding stages like `encoder`, you can initialize `decoder`\n",
        "            if self.save_stages:\n",
        "                # if you want to save the stages, initialize decoder\n",
        "                decoder = []  # initialize it here if you want to store the decoding stages\n",
        "                decoder.append(stage_probs)\n",
        "\n",
        "        return decoded_msg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_1_3_'></a>[Lempel-Ziv-Welch (LZW) Coding](#toc0_)\n",
        "\n",
        "- LZW is widely used in formats like **GIF** and **TIFF** and works by **encoding input data into variable-length codes**, utilizing a **dynamic dictionary** to represent repeated sequences.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [**A Technique for High-Performance Data Compression**](https://www.computer.org/csdl/magazine/co/1984/06/01659158/13rRUwIF63T) by [Terry A. Welch](https://www.computer.org/csdl/search/default?type=author&givenName=T.A.&surname=Welch) in *1984*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LZW:\n",
        "    def __init__(self):\n",
        "        self.reset_dictionary()\n",
        "\n",
        "    def reset_dictionary(self):\n",
        "        \"\"\"Reset the dictionary to initial state\"\"\"\n",
        "        # Initial dictionary contains all possible single-byte values\n",
        "        self.dictionary = {}\n",
        "        self.reverse_dictionary = {}\n",
        "        self.next_code = 0\n",
        "\n",
        "        # Initialize with single character patterns\n",
        "        for i in range(256):\n",
        "            self._add_to_dictionary(bytes([i]))\n",
        "\n",
        "    def _add_to_dictionary(self, pattern):\n",
        "        \"\"\"Add a new pattern to the dictionary\"\"\"\n",
        "        if pattern not in self.dictionary:\n",
        "            self.dictionary[pattern] = self.next_code\n",
        "            self.reverse_dictionary[self.next_code] = pattern\n",
        "            self.next_code += 1\n",
        "\n",
        "    def encode(self, data):\n",
        "        \"\"\"\n",
        "        Compress a list of integers (0-255) using LZW algorithm\n",
        "        \"\"\"\n",
        "        # Convert input data to bytes\n",
        "        input_bytes = bytes(data)\n",
        "\n",
        "        self.reset_dictionary()\n",
        "        compressed = []\n",
        "        w = bytes()\n",
        "\n",
        "        for c in input_bytes:\n",
        "            c_bytes = bytes([c])\n",
        "            wc = w + c_bytes\n",
        "            if wc in self.dictionary:\n",
        "                w = wc\n",
        "            else:\n",
        "                compressed.append(self.dictionary[w])\n",
        "                self._add_to_dictionary(wc)\n",
        "                w = c_bytes\n",
        "\n",
        "        if w:\n",
        "            compressed.append(self.dictionary[w])\n",
        "\n",
        "        return compressed\n",
        "\n",
        "    def decode(self, compressed):\n",
        "        \"\"\"\n",
        "        Decompress a list of code words using LZW algorithm\n",
        "        \"\"\"\n",
        "        self.reset_dictionary()\n",
        "        decompressed = []\n",
        "\n",
        "        if not compressed:\n",
        "            return decompressed\n",
        "\n",
        "        # Convert first code\n",
        "        old_code = compressed[0]\n",
        "        decompressed.extend(self.reverse_dictionary[old_code])\n",
        "        s = self.reverse_dictionary[old_code]\n",
        "\n",
        "        for code in compressed[1:]:\n",
        "            if code in self.reverse_dictionary:\n",
        "                entry = self.reverse_dictionary[code]\n",
        "            elif code == self.next_code:\n",
        "                entry = s + bytes([s[0]])\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid compressed code: {code}\")\n",
        "\n",
        "            decompressed.extend(entry)\n",
        "\n",
        "            # Add to dictionary\n",
        "            new_pattern = s + bytes([entry[0]])\n",
        "            self._add_to_dictionary(new_pattern)\n",
        "\n",
        "            s = entry\n",
        "\n",
        "        return list(decompressed)\n",
        "\n",
        "    def print_dictionary(self, reverse=False):\n",
        "        \"\"\"Print the current dictionary contents\"\"\"\n",
        "        print(\"\\nCurrent Dictionary:\")\n",
        "        if reverse:\n",
        "            for code, pattern in sorted(self.reverse_dictionary.items()):\n",
        "                print(f\"{code:4}: {list(pattern)}\")\n",
        "        else:\n",
        "            for pattern, code in sorted(self.dictionary.items(), key=lambda x: x[1]):\n",
        "                print(f\"{list(pattern)}: {code:4}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lzw_total_bits(lzw_enc, initial_dict=256, max_bits=12):\n",
        "    \"\"\"Calculate total bits including header and payload.\"\"\"\n",
        "    # 1. header bits (assumes 4-byte header)\n",
        "    header_bits = 4 * 8  # e.g., 32 bits for metadata\n",
        "\n",
        "    # 2. payload bits (variable-width codes)\n",
        "    bit_width = 9  # starts at 9 bits\n",
        "    dict_size = initial_dict  # starts at 256\n",
        "    payload_bits = 0\n",
        "\n",
        "    for code in lzw_enc:\n",
        "        payload_bits += bit_width\n",
        "        if code >= dict_size and bit_width < max_bits:\n",
        "            dict_size *= 2\n",
        "            bit_width += 1\n",
        "\n",
        "    return header_bits + payload_bits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lzw = LZW()\n",
        "lzw_enc = lzw.encode(im_6_flatten)\n",
        "lzw_dec = lzw.decode(lzw_enc)\n",
        "im_6_dec = np.array(lzw_dec, dtype=np.uint8).reshape(im_6.shape)\n",
        "is_equal = np.array_equal(im_6, im_6_dec)\n",
        "\n",
        "# log\n",
        "print(f\"im_6     (bitstream length)      : {int(np.prod(im_6.shape) * np.log2(np.iinfo(im_6.dtype).max + 1))} bits\")\n",
        "print(f\"lzw_enc  (bitstream length)      : {lzw_total_bits(lzw_enc)} bits\")\n",
        "print(f\"entropy  (avg. bitstream length) : {calculate_entropy(im_6) * np.prod(im_6.shape)} bits\")\n",
        "print(f\"is_equal (lossless)              : {is_equal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# show dictionary\n",
        "lzw.print_dictionary(reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_2_'></a>[Interpixel (Spatial) Redundancy](#toc0_)\n",
        "\n",
        "- It refers to the redundancy found in the spatial domain of an image.\n",
        "- It occurs when **neighboring pixels** in an image **share similar values**, meaning there is a **pattern** or **correlation** between them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_2_1_'></a>[Run-Length Encoding (RLE)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RunLengthEncoder:\n",
        "    def __init__(self):\n",
        "        self.encoded = []\n",
        "\n",
        "    def encode(self, flat_list: list):\n",
        "        if not flat_list:\n",
        "            self.encoded = []\n",
        "            return self.encoded\n",
        "\n",
        "        self.encoded = []\n",
        "        prev = flat_list[0]\n",
        "        count = 1\n",
        "\n",
        "        for val in flat_list[1:]:\n",
        "            if val == prev:\n",
        "                count += 1\n",
        "            else:\n",
        "                self.encoded.append((prev, count))\n",
        "                prev = val\n",
        "                count = 1\n",
        "        self.encoded.append((prev, count))\n",
        "        return self.encoded\n",
        "\n",
        "    def decode(self):\n",
        "        return [val for val, count in self.encoded for _ in range(count)]\n",
        "\n",
        "    def get_table(self):\n",
        "        return self.encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_rle_size(encoded, value_dtype=np.uint8, count_dtype=np.uint16):\n",
        "    val_bits = np.iinfo(value_dtype).bits\n",
        "    count_bits = np.iinfo(count_dtype).bits\n",
        "    return len(encoded) * (val_bits + count_bits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rle = RunLengthEncoder()\n",
        "rle_enc = rle.encode(im_6_flatten)\n",
        "rle_dec = rle.decode()\n",
        "im_6_dec = np.array(rle_dec, dtype=np.uint8).reshape(im_6.shape)\n",
        "is_equal = np.array_equal(im_6, im_6_dec)\n",
        "\n",
        "# log\n",
        "print(f\"im_6     (bitstream length)      : {int(np.prod(im_6.shape) * np.log2(np.iinfo(im_6.dtype).max + 1))} bits\")\n",
        "print(\n",
        "    f\"rle_enc  (bitstream length)      : {calculate_rle_size(rle_enc, value_dtype=im_6.dtype, count_dtype=np.uint16)} bits\"\n",
        ")\n",
        "print(f\"entropy  (avg. bitstream length) : {calculate_entropy(im_6) * np.prod(im_6.shape)} bits\")\n",
        "print(f\"is_equal (lossless)              : {is_equal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# run-length codebook\n",
        "print(\"\\nrune-length codebook:\")\n",
        "for symbol, length in rle.get_table():\n",
        "    print(f\"symbol: {symbol:3} -> length: {length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_2_2_'></a>[Differential Encoding](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_2_3_'></a>[Predictive Coding](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_2_4_'></a>[Transform Coding](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_2_5_'></a>[Wavelet Transform](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_2_6_'></a>[Block-based Compression](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_3_'></a>[Temporal Redundancy](#toc0_)\n",
        "\n",
        "- It refers to the redundancy that exists between **successive frames** in a **sequence** of images or video.\n",
        "- This type of redundancy arises because **adjacent frames** (in a video) or successive images (in a time-series context) often contain very **similar information**, making them **predictable** or **compressible**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_1_'></a>[Inter-frame Compression (Predictive Compression)](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_2_'></a>[Motion Compensation](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_3_'></a>[Differential Encoding](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_4_'></a>[Keyframe Extraction](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_5_'></a>[Temporal Filtering](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_6_'></a>[Block-based Motion Estimation](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_3_7_'></a>[Long-Term Prediction](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc4_4_'></a>[Psychovisual Redundancy](#toc0_)\n",
        "\n",
        "- It refers to the redundancy in image or video data that arises due to the **limitations of the human visual system (HVS)**.\n",
        "- In essence, certain **details** in images or video frames may be **less perceptible** to the **human eye**.\n",
        "- They can be **removed or simplified without significantly affecting the perceived quality** of the image or video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_4_1_'></a>[Chrominance Subsampling](#toc0_)\n",
        "\n",
        "- It **reduces** the resolution of color (**Cb** and **Cr**) information in images.\n",
        "- This technique is commonly used in **JPEG**, **MPEG**, and other **lossy compression** standards.\n",
        "\n",
        "üß† **Conceptual Basis**\n",
        "\n",
        "Subsampling ratios are expressed as **J:a:b** (typically 4:a:b), where:\n",
        "\n",
        "- `J` is the number of **luma (Y)** samples in a reference row (usually 4).\n",
        "- `a` is the number of **chroma samples (Cb and Cr)** in the **first row** for every 4 Y samples.\n",
        "- `b` is the number of **chroma samples** in the **second row**, indicating vertical subsampling.\n",
        "\n",
        "üìä **Common Subsampling Ratios**\n",
        "\n",
        "<figure style=\"text-align:center; margin:0;\">\n",
        "  <img src=\"../../assets/images/original/vector/compression/chroma-subsampling.svg\" alt=\"chroma-subsampling.svg\" style=\"max-width:80%; height:auto;\">\n",
        "  <figcaption>Common Chrominance Subsampling Ratios</figcaption>\n",
        "</figure>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_2_yuv = cv2.cvtColor(im_2, cv2.COLOR_RGB2YUV)\n",
        "im_2_y = im_2_yuv[:, :, 0]\n",
        "im_2_u = im_2_yuv[:, :, 1]\n",
        "im_2_v = im_2_yuv[:, :, 2]\n",
        "\n",
        "# log\n",
        "print(f\"im_2_yuv.shape: {im_2_yuv.shape}\\n\")\n",
        "print(f\"Y[:2, :4]:\\n{im_2_y[:2, :4]}\\n\")\n",
        "print(f\"U[:2, :4]:\\n{im_2_u[:2, :4]}\\n\")\n",
        "print(f\"V[:2, :4]:\\n{im_2_v[:2, :4]}\")\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 4), layout=\"compressed\")\n",
        "axs[0].imshow(im_2)\n",
        "axs[0].set_title(\"RGB image\")\n",
        "axs[1].imshow(im_2_y, cmap=\"gray\")\n",
        "axs[1].set_title(\"Y channel\")\n",
        "axs[2].imshow(im_2_u, cmap=\"seismic\")\n",
        "axs[2].set_title(\"U channel\")\n",
        "axs[3].imshow(im_2_v, cmap=\"seismic\")\n",
        "axs[3].set_title(\"V channel\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4:2:2\n",
        "im_2_u_422 = im_2_u[:, ::2]\n",
        "im_2_v_422 = im_2_v[:, ::2]\n",
        "\n",
        "# reconstruct\n",
        "U_422_rec = cv2.resize(im_2_u_422, im_2_yuv.shape[:-1][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "V_422_rec = cv2.resize(im_2_v_422, im_2_yuv.shape[:-1][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "im_2_yuv_422 = np.dstack([im_2_y, U_422_rec, V_422_rec])\n",
        "\n",
        "# log\n",
        "print(f\"U_422.shape: {im_2_u_422.shape}\")\n",
        "print(f\"V_422.shape: {im_2_v_422.shape}\\n\")\n",
        "print(f\"U_422[:2, :4]:\\n{im_2_u_422[:2, :4]}\\n\")\n",
        "print(f\"V_422[:2, :4]:\\n{im_2_v_422[:2, :4]}\")\n",
        "print(f\"-\" * 50)\n",
        "print(f\"U_422_rec.shape: {U_422_rec.shape}\")\n",
        "print(f\"V_422_rec.shape: {V_422_rec.shape}\\n\")\n",
        "print(f\"U_422_rec[:2, :4]:\\n{U_422_rec[:2, :4]}\\n\")\n",
        "print(f\"V_422_rec[:2, :4]:\\n{V_422_rec[:2, :4]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4:2:0\n",
        "im_2_u_420 = im_2_u[::2, ::2]\n",
        "im_2_v_420 = im_2_v[::2, ::2]\n",
        "\n",
        "# reconstruct\n",
        "U_420_rec = cv2.resize(im_2_u_420, im_2_yuv.shape[:-1][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "V_420_rec = cv2.resize(im_2_v_420, im_2_yuv.shape[:-1][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "im_2_yuv_420 = np.dstack([im_2_y, U_420_rec, V_420_rec])\n",
        "\n",
        "# log\n",
        "print(f\"U_420.shape: {im_2_u_420.shape}\")\n",
        "print(f\"V_420.shape: {im_2_v_420.shape}\\n\")\n",
        "print(f\"U_420[:2, :4]:\\n{im_2_u_420[:2, :4]}\\n\")\n",
        "print(f\"V_420[:2, :4]:\\n{im_2_v_420[:2, :4]}\")\n",
        "print(f\"-\" * 50)\n",
        "print(f\"U_420_rec.shape: {U_420_rec.shape}\")\n",
        "print(f\"V_420_rec.shape: {V_420_rec.shape}\\n\")\n",
        "print(f\"U_420_rec[:2, :4]:\\n{U_420_rec[:2, :4]}\\n\")\n",
        "print(f\"V_420_rec[:2, :4]:\\n{V_420_rec[:2, :4]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4:1:1\n",
        "im_2_u_411 = im_2_u[:, ::4]\n",
        "im_2_v_411 = im_2_v[:, ::4]\n",
        "\n",
        "# reconstruct\n",
        "U_411_rec = cv2.resize(im_2_u_411, im_2_yuv.shape[:-1][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "V_411_rec = cv2.resize(im_2_v_411, im_2_yuv.shape[:-1][::-1], interpolation=cv2.INTER_LINEAR)\n",
        "im_2_yuv_411 = np.dstack([im_2_y, U_411_rec, V_411_rec])\n",
        "\n",
        "# log\n",
        "print(f\"U_411.shape: {im_2_u_411.shape}\")\n",
        "print(f\"V_411.shape: {im_2_v_411.shape}\\n\")\n",
        "print(f\"U_411[:2, :4]:\\n{im_2_u_411[:2, :4]}\\n\")\n",
        "print(f\"V_411[:2, :4]:\\n{im_2_v_411[:2, :4]}\")\n",
        "print(f\"-\" * 50)\n",
        "print(f\"U_411_rec.shape: {U_411_rec.shape}\")\n",
        "print(f\"V_411_rec.shape: {V_411_rec.shape}\\n\")\n",
        "print(f\"U_411_rec[:2, :4]:\\n{U_411_rec[:2, :4]}\\n\")\n",
        "print(f\"V_411_rec[:2, :4]:\\n{V_411_rec[:2, :4]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_2_rgb_422 = cv2.cvtColor(im_2_yuv_422, code=cv2.COLOR_YUV2RGB)\n",
        "im_2_rgb_420 = cv2.cvtColor(im_2_yuv_420, code=cv2.COLOR_YUV2RGB)\n",
        "im_2_rgb_411 = cv2.cvtColor(im_2_yuv_411, code=cv2.COLOR_YUV2RGB)\n",
        "\n",
        "# plot\n",
        "images = [im_2, im_2_rgb_422, im_2_rgb_420, im_2_rgb_411]\n",
        "titles = [\"Original RGB Image\", \"RGB 4:2:2\", \"RGB 4:2:0\", \"RGB 4:1:1\"]\n",
        "zoom_images = [\n",
        "    im_2[220:300, 220:300],\n",
        "    im_2_rgb_422[220:300, 220:300],\n",
        "    im_2_rgb_420[220:300, 220:300],\n",
        "    im_2_rgb_411[220:300, 220:300],\n",
        "]\n",
        "zoom_titles = [\"Zoomed Original\", \"Zoomed RGB 4:2:2\", \"Zoomed RGB 4:2:0\", \"Zoomed RGB 4:1:1\"]\n",
        "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(16, 8), layout=\"compressed\")\n",
        "for i in range(4):\n",
        "    axs[0, i].imshow(images[i])\n",
        "    axs[0, i].set_title(titles[i])\n",
        "    axs[0, i].axis(\"off\")\n",
        "for i in range(4):\n",
        "    axs[1, i].imshow(zoom_images[i])\n",
        "    axs[1, i].set_title(zoom_titles[i])\n",
        "    axs[1, i].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_4_2_'></a>[Quantization](#toc0_)\n",
        "\n",
        "- After DCT, image data is represented as frequency coefficients.\n",
        "- Since the human eye is less sensitive to high-frequency components, quantization reduces their precision (i.e., more aggressive rounding), often to zero.\n",
        "- This enables better compression with minimal perceived quality loss\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- [**Digital Compression and Coding of Continuous-Tone Still Images (JPEG Standard, ITU-T T.81)**](https://www.w3.org/Graphics/JPEG/itu-t81.pdf) by [Joint Photographic Experts Group (JPEG)](https://jpeg.org/about.html) in *1992*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qm_luma = np.array(\n",
        "    [\n",
        "        [16, 11, 10, 16, 24, 40, 51, 61],\n",
        "        [12, 12, 14, 19, 26, 58, 60, 55],\n",
        "        [14, 13, 16, 24, 40, 57, 69, 56],\n",
        "        [14, 17, 22, 29, 51, 87, 80, 62],\n",
        "        [18, 22, 37, 56, 68, 109, 103, 77],\n",
        "        [24, 35, 55, 64, 81, 104, 113, 92],\n",
        "        [49, 64, 78, 87, 103, 121, 120, 101],\n",
        "        [72, 92, 95, 98, 112, 100, 103, 99],\n",
        "    ],\n",
        "    dtype=np.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qm_chroma = np.array(\n",
        "    [\n",
        "        [17, 18, 24, 47, 99, 99, 99, 99],\n",
        "        [18, 21, 26, 66, 99, 99, 99, 99],\n",
        "        [24, 26, 56, 99, 99, 99, 99, 99],\n",
        "        [47, 66, 99, 99, 99, 99, 99, 99],\n",
        "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
        "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
        "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
        "        [99, 99, 99, 99, 99, 99, 99, 99],\n",
        "    ],\n",
        "    dtype=np.float32,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# this formula comes from libjpeg, the de facto standard JPEG library, and is used by most open-source image software, such as GIMP, Pillow (PIL), ImageMagick, and OpenCV\n",
        "def scale_quant_matrix(Q, quality):\n",
        "    assert 1 <= quality <= 100, \"Quality must be between 1 and 100\"\n",
        "\n",
        "    if quality < 50:\n",
        "        scale = 5000 / quality\n",
        "    else:\n",
        "        scale = 200 - 2 * quality\n",
        "\n",
        "    scaled_matrix = np.floor((Q * scale + 50) / 100)\n",
        "\n",
        "    # Clip values to 1-255 range as per JPEG spec\n",
        "    scaled_matrix = np.clip(scaled_matrix, 1, 255)\n",
        "\n",
        "    return scaled_matrix\n",
        "\n",
        "\n",
        "# scale quantization matrix\n",
        "qm_luma_qf80 = scale_quant_matrix(qm_luma, quality=80)\n",
        "qm_chroma_qf80 = scale_quant_matrix(qm_chroma, quality=80)\n",
        "\n",
        "# log\n",
        "print(f\"qm_luma_qf80:\\n{qm_luma_qf80}\\n\")\n",
        "print(f\"qm_chroma_qf80:\\n{qm_chroma_qf80}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert rgb to YCbCr\n",
        "im_2_y = cv2.cvtColor(im_2, code=cv2.COLOR_RGB2YCrCb)[:, :, 0].astype(np.float32)\n",
        "\n",
        "# scale Y to full range\n",
        "im_2_y = (im_2_y - 16) * (255 / 219)\n",
        "\n",
        "# extract first batch and ensure it is float32 to avoid underflow\n",
        "im_2_patch00 = im_2_y[:8, :8].astype(np.float32)\n",
        "\n",
        "# DCT\n",
        "im_2_patch00_dct2 = sp.fftpack.dctn(im_2_patch00 - 128, norm=\"ortho\")\n",
        "\n",
        "# quantization (JPEG-style: floor(x + 0.5))\n",
        "im_2_patch00_dct2_quantize = np.floor(im_2_patch00_dct2 / qm_luma_qf80 + 0.5)\n",
        "\n",
        "# dequantization\n",
        "im_2_patch00_dct2_dequantize = im_2_patch00_dct2_quantize * qm_luma_qf80\n",
        "\n",
        "# IDCT\n",
        "im_2_patch00_idct2 = sp.fftpack.idctn(im_2_patch00_dct2_dequantize, norm=\"ortho\")\n",
        "\n",
        "# shift back to [0, 255] and clip\n",
        "im_2_patch00_idct2 += 128\n",
        "im_2_patch00_idct2 = np.clip(im_2_patch00_idct2, 0, 255).astype(np.uint8)\n",
        "\n",
        "# log\n",
        "print(f\"im_2_patch00:\\n{im_2_patch00.astype(np.uint8)}\\n\")\n",
        "print(f\"im_2_patch00_idct2:\\n{im_2_patch00_idct2}\")\n",
        "\n",
        "# plot\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8, 4), layout=\"compressed\")\n",
        "axs[0].imshow(im_2_patch00, cmap=\"gray\", vmin=0, vmax=255)\n",
        "axs[0].set_title(\"Original 8x8 patch\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].imshow(im_2_patch00_idct2, cmap=\"gray\", vmin=0, vmax=255)\n",
        "axs[1].set_title(\"Reconstructed 8x8 patch\")\n",
        "axs[1].axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_4_3_'></a>[Perceptual Coding](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc4_4_4_'></a>[Rate-Distortion Optimization](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "author_name": "Amirhossein Heydari",
    "kernelspec": {
      "display_name": "media-processing-workshop-sxUc00b2-py3.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
