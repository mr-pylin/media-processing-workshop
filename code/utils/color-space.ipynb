{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4;\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/media-processing-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"display: flex; justify-content: flex-end; flex: 1; gap: 8px; align-items: center; padding: 0;\">\n",
    "        <a href=\"https://opencv.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/opencv/logo/OpenCV_logo_no_text-1.svg\"\n",
    "                 alt=\"OpenCV Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://pillow.readthedocs.io/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/pillow/logo/pillow-logo-248x250.png\"\n",
    "                 alt=\"PIL Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scikit-image.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/scikit-image/logo/logo.png\"\n",
    "                 alt=\"scikit-image Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scipy.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/scipy/logo/logo.svg\"\n",
    "                 alt=\"SciPy Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Load Images](#toc2_)    \n",
    "- [Color Space](#toc3_)    \n",
    "  - [Color Theory Fundamentals](#toc3_1_)    \n",
    "    - [Human Vision Basics](#toc3_1_1_)    \n",
    "    - [Human Color Gamut](#toc3_1_2_)    \n",
    "  - [Color Space Conversion](#toc3_2_)    \n",
    "    - [RGB - Grayscale](#toc3_2_1_)    \n",
    "      - [Manual](#toc3_2_1_1_)    \n",
    "      - [Using OpenCV](#toc3_2_1_2_)    \n",
    "      - [Using PIL](#toc3_2_1_3_)    \n",
    "      - [Using Scikit-Image](#toc3_2_1_4_)    \n",
    "      - [Comparison of Methods](#toc3_2_1_5_)    \n",
    "    - [RGB - BGR](#toc3_2_2_)    \n",
    "      - [Manual](#toc3_2_2_1_)    \n",
    "      - [Using OpenCV](#toc3_2_2_2_)    \n",
    "    - [RGB - YUV](#toc3_2_3_)    \n",
    "      - [Manual](#toc3_2_3_1_)    \n",
    "      - [Using OpenCV](#toc3_2_3_2_)    \n",
    "    - [RGB - YCbCr (YCC)](#toc3_2_4_)    \n",
    "      - [Manual](#toc3_2_4_1_)    \n",
    "      - [Using OpenCV](#toc3_2_4_2_)    \n",
    "    - [RGB - HSV](#toc3_2_5_)    \n",
    "      - [Using OpenCV](#toc3_2_5_1_)    \n",
    "    - [RGB - CMYK](#toc3_2_6_)    \n",
    "      - [Using PIL](#toc3_2_6_1_)    \n",
    "  - [Linear Color Transformations in RGB](#toc3_3_)    \n",
    "    - [Warming Filters](#toc3_3_1_)    \n",
    "      - [Sepia](#toc3_3_1_1_)    \n",
    "      - [Rosewood](#toc3_3_1_2_)    \n",
    "      - [Amber / Golden](#toc3_3_1_3_)    \n",
    "      - [Sunset / Autumn](#toc3_3_1_4_)    \n",
    "    - [Cooling Filters](#toc3_3_2_)    \n",
    "      - [Cool Blue](#toc3_3_2_1_)    \n",
    "      - [Cyan Tint](#toc3_3_2_2_)    \n",
    "      - [Moonlight](#toc3_3_2_3_)    \n",
    "  - [Nonlinear Color and Photographic Effects](#toc3_4_)    \n",
    "    - [Monochromatic Filters](#toc3_4_1_)    \n",
    "      - [Duotone](#toc3_4_1_1_)    \n",
    "      - [Tritone](#toc3_4_1_2_)    \n",
    "    - [Photographic / Special Effects](#toc3_4_2_)    \n",
    "      - [Solarize](#toc3_4_2_1_)    \n",
    "  - [Visualize using Custom Color Maps](#toc3_5_)    \n",
    "    - [Indexed Image](#toc3_5_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable automatic figure display (plt.show() required)  \n",
    "# this ensures consistency with .py scripts and gives full control over when plots appear\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set NumPy arrays to print wider lines (120 chars)\n",
    "np.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Load Images](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1 = Image.open(fp=\"../../assets/images/misc/lenna_rgba_indexed.png\")\n",
    "image_2 = Image.open(fp=\"../../assets/images/wikipedia/Edvard_Munch-The_Scream-National_Gallery_of_Norway.jpg\")\n",
    "image_3 = Image.open(fp=\"../../assets/images/wikipedia/Van_Gogh-Starry_Night-Google_Art_Project.jpg\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "axs[0].imshow(image_1, vmin=0, vmax=255)\n",
    "axs[0].set_title(\"Lenna - Indexed RGBA\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(image_2, vmin=0, vmax=255)\n",
    "axs[1].set_title(\"The Scream - RGB\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[2].imshow(image_3, vmin=0, vmax=255)\n",
    "axs[2].set_title(\"Starry Night - RGB\")\n",
    "axs[2].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIL.Image.Image to np.ndarray\n",
    "im_1 = np.array(image_1)\n",
    "im_2 = np.array(image_2)\n",
    "im_3 = np.array(image_3)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_2, im_2[:, :, 0], im_2[:, :, 1], im_2[:, :, 2]]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if len(img.shape) == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Color Space](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Color Theory Fundamentals](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[Human Vision Basics](#toc0_)\n",
    "\n",
    "- **Spectral range**: 380-700nm  \n",
    "- **Color discrimination**: ~10-11 million colors (optimal conditions)  \n",
    "- **Cone types**: S (420nm), M (530nm), L (560nm) peak sensitivity\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 40px; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/Cone-fundamentals-with-srgb-spectrum.svg\"\n",
    "         alt=\"Cone-fundamentals-with-srgb-spectrum.svg\"\n",
    "         style=\"max-height: 250px; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">Normalized responsivity spectra of human cone cells, S, M, and L types</div>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/ConeMosaics.png\"\n",
    "         alt=\"ConeMosaics.png\"\n",
    "         style=\"max-height: 250px; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">Distribution of cone cells in the fovea of an individual with normal color vision (left), and a color blind (protanopic) retina</div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; margin-top: 15px;\">\n",
    "  Visual representation of cone cell characteristics and distribution (¬©Ô∏è \n",
    "  <a href=\"https://en.wikipedia.org/wiki/Cone_cell\">Wikipedia - Cone cell</a>)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Human Color Gamut](#toc0_)\n",
    "\n",
    "The **human color gamut** represents all colors perceivable by human vision.  \n",
    "It is commonly visualized using the **CIE 1931 chromaticity diagram**.  \n",
    "\n",
    "> Approximately ~8% of males and ~0.5% of females are affected by color blindness, which alters how they perceive the gamut.\n",
    "\n",
    "<figure style=\"text-align: center; margin: 0;\">\n",
    "  <div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <div style=\"flex: 1; background-color: lightgray; padding: 10px; display: flex; align-items: center; justify-content: center;\">\n",
    "      <img src=\"../../assets/images/third_party/Rechteckspektrum_sRGB.svg\"\n",
    "           alt=\"Rechteckspektrum_sRGB.svg\"\n",
    "           style=\"max-width: 100%; height: auto;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; background-color: lightgray; padding: 10px; display: flex; align-items: center; justify-content: center;\">\n",
    "      <img src=\"../../assets/images/third_party/Cie_Chart_with_sRGB_gamut_by_spigget.png\"\n",
    "           alt=\"Cie_Chart_with_sRGB_gamut_by_spigget.png\"\n",
    "           style=\"max-width: 80%; height: auto;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; background-color: lightgray; padding: 10px; display: flex; align-items: center; justify-content: center;\">\n",
    "      <img src=\"../../assets/images/third_party/CIE1931xy_gamut_comparison.svg\"\n",
    "           alt=\"CIE1931xy_gamut_comparison.svg\"\n",
    "           style=\"max-width: 90%; height: auto;\">\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Visual representations of color gamuts in sRGB space (¬©Ô∏è \n",
    "    <a href=\"https://en.wikipedia.org/wiki/Gamut\">Wikipedia - Gamut</a>)\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "**CIE Coordinates (Simplified DIP Approach):**\n",
    "\n",
    "Chromaticity coordinates can be simplified in DIP as:\n",
    "\n",
    "- **x**: Red component\n",
    "- **y**: Green component  \n",
    "- **z**: Blue component (z = 1-x-y)\n",
    "\n",
    "**Example**:  \n",
    "780nm wavelength ‚Üí x=0.74, y=0.26, z=0.00 (deep red)\n",
    "\n",
    "---\n",
    "\n",
    "**RGB Display Limitations:**\n",
    "\n",
    "Modern displays reproduce only a **subset of human-visible colors**:\n",
    "\n",
    "- **sRGB**: ~35% coverage\n",
    "- **Adobe RGB**: ~50% coverage\n",
    "- **DCI-P3**: ~45% coverage\n",
    "\n",
    "---\n",
    "\n",
    "**2D vs 3D Color Space:**\n",
    "\n",
    "- **2D diagram**: Chromaticity (hue/saturation) at constant luminance\n",
    "- **3D space**: Adds brightness dimension (Y-value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Color Space Conversion](#toc0_)\n",
    "\n",
    "- Different tasks often require images in spaces other than RGB, such as visualization, compression, or enhancement.  \n",
    "- Some spaces, like YCbCr, separate luminance and chrominance for more efficient processing.  \n",
    "- Understanding color spaces is essential for building effective image processing pipelines.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `cv2.cvtColor`: [docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#gaf86c09fe702ed037c03c2bc603ceab14](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#gaf86c09fe702ed037c03c2bc603ceab14)\n",
    "- Color Space Conversions [`cv2`]: [docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html)\n",
    "- Modes [`PIL`]: [pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes)\n",
    "- Colormap reference [`matplotlib`]: [matplotlib.org/stable/gallery/color/colormap_reference.html](https://matplotlib.org/stable/gallery/color/colormap_reference.html)\n",
    "- ITU-R Recommendation BT.601-7 [an international technical standard]: [itu.int/dms_pubrec/itu-r/rec/bt/r-rec-bt.601-7-201103-i!!pdf-e.pdf](http://itu.int/dms_pubrec/itu-r/rec/bt/r-rec-bt.601-7-201103-i!!pdf-e.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[RGB - Grayscale](#toc0_)\n",
    "\n",
    "Converting an RGB image to grayscale reduces color information to shades of gray.  \n",
    "This simplification is useful for several reasons:\n",
    "\n",
    "- **Data Reduction:**  \n",
    "  Only one channel is stored instead of three, reducing memory and computational requirements.\n",
    "\n",
    "- **Preprocessing for Computer Vision:**  \n",
    "  Many algorithms (e.g., edge detection, segmentation, pattern recognition) work better or only on grayscale images.\n",
    "\n",
    "- **Highlighting Intensity Information:**  \n",
    "  Luminance variations often correspond more closely to structural details than color variations.\n",
    "\n",
    "- **Simplifying Analysis:**  \n",
    "  In medical imaging, document scanning, or texture analysis, grayscale helps isolate shape, contrast, and texture without color distractions.\n",
    "\n",
    "---\n",
    "\n",
    "üî¢ Conversion Methods\n",
    "\n",
    "1. **Luminance (Perceptual Weights, Standard Method) ‚úÖ:**  \n",
    "   Human vision is more sensitive to green than red or blue.  \n",
    "   Weighted sum formula:\n",
    "\n",
    "   $$\n",
    "   Y = 0.299 \\, R + 0.587 \\, G + 0.114 \\, B\n",
    "   $$\n",
    "\n",
    "   - **Use when:** Accurate brightness representation is needed, e.g., human viewing, compression, perceptual tasks.\n",
    "\n",
    "1. **Averaging Method:**  \n",
    "   Arithmetic mean of the three channels:\n",
    "\n",
    "   $$\n",
    "   Y = \\frac{R + G + B}{3}\n",
    "   $$\n",
    "\n",
    "   - **Use when:** Simplicity is more important than perceptual accuracy, e.g., quick visualization, basic preprocessing.\n",
    "\n",
    "1. **L2 Norm (Euclidean Norm Method):**  \n",
    "   Computes the magnitude of the RGB vector:\n",
    "\n",
    "   $$\n",
    "   Y = \\sqrt{\\frac{R^2 + G^2 + B^2}{3}}\n",
    "   $$\n",
    "\n",
    "   - **Use when:** Emphasizing overall intensity magnitude or treating RGB as a feature vector in scientific imaging.\n",
    "\n",
    "1. **Max/Min Channel Method:**  \n",
    "   Take the maximum or minimum channel value:\n",
    "\n",
    "   $$\n",
    "   Y = \\max(R, G, B) \\quad \\text{or} \\quad Y = \\min(R, G, B)\n",
    "   $$\n",
    "\n",
    "   - **Use when:** Highlighting the strongest or weakest channel for edge detection or contrast emphasis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_GS = np.array(\n",
    "    [\n",
    "        [0.299, 0.587, 0.114],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# approximation using pseudo-inverse [best linear least-squares approximation]\n",
    "GS_TO_RGB = np.linalg.pinv(RGB_TO_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gs(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_GS.T).reshape(image.shape[:2])\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def gs_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    rgb = (image.reshape(-1, 1) @ GS_TO_RGB.T).reshape(*image.shape, 3)\n",
    "    return np.clip(rgb, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_gs_1  = rgb_to_gs(im_2)\n",
    "gs_1_to_rgb_1 = gs_to_rgb(im_2_to_gs_1)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (best approximation)\"]\n",
    "images = [im_2, im_2_to_gs_1, gs_1_to_rgb_1]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_gs_2 = cv2.cvtColor(im_2, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# replicates the grayscale channel into all three RGB channels (still grayish image)\n",
    "gs_2_to_rgb_2 = cv2.cvtColor(im_2_to_gs_2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (repeats luma channel)\"]\n",
    "images = [im_2, im_2_to_gs_2, gs_2_to_rgb_2]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_3_'></a>[Using PIL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_gs_3 = Image.fromarray(im_2).convert(\"L\")\n",
    "gs_3_to_rgb_3 = im_2_to_gs_3.convert(\"RGB\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (repeats luma channel)\"]\n",
    "images = [im_2, im_2_to_gs_3, gs_3_to_rgb_3]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_4_'></a>[Using Scikit-Image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_gs_4 = ski.color.rgb2gray(im_2)\n",
    "gs_4_to_rgb_4 = ski.color.gray2rgb(im_2_to_gs_4)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (repeats luma channel)\"]\n",
    "images = [im_2, im_2_to_gs_4, gs_4_to_rgb_4]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_5_'></a>[Comparison of Methods](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = im_2[:, :, 0].astype(np.float32)\n",
    "G = im_2[:, :, 1].astype(np.float32)\n",
    "B = im_2[:, :, 2].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. luminance (perceptual weights)\n",
    "im_2_method_1 = 0.299 * R + 0.587 * G + 0.114 * B\n",
    "im_2_method_1 = np.clip(im_2_method_1, 0, 255).astype(np.uint8)\n",
    "\n",
    "# 2. averaging method\n",
    "im_2_method_2 = (R + G + B) / 3\n",
    "im_2_method_2 = np.clip(im_2_method_2, 0, 255).astype(np.uint8)\n",
    "\n",
    "# 3. L2 norm (euclidean norm)\n",
    "im_2_method_3 = np.sqrt((R**2 + G**2 + B**2) / 3)\n",
    "im_2_method_3 = np.clip(im_2_method_3, 0, 255).astype(np.uint8)\n",
    "\n",
    "# 4. max/min channel method\n",
    "im_2_method_4_1 = np.maximum(np.maximum(R, G), B).astype(np.uint8)\n",
    "im_2_method_4_2 = np.minimum(np.minimum(R, G), B).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 6, figsize=(24, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"Perceptual Weights\", \"Averaging\", \"L2 Norm\", \"Max Channel\", \"Min Channel\"]\n",
    "images = [im_2, im_2_method_1, im_2_method_2, im_2_method_3, im_2_method_4_1, im_2_method_4_2]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[RGB - BGR](#toc0_)\n",
    "\n",
    "OpenCV uses the BGR (Blue, Green, Red) channel order as its default color format instead of the more common RGB order for historical and compatibility reasons:\n",
    "\n",
    "- **Historical Legacy:**  \n",
    "  Early versions of OpenCV were designed to be compatible with the Windows bitmap (BMP) image format, which stores pixel data in BGR order. Using BGR natively simplified reading and writing BMP files without additional channel reordering.\n",
    "\n",
    "- **Performance Considerations:**  \n",
    "  By matching the underlying image formats and memory layouts commonly used on Windows systems, OpenCV avoids unnecessary data copying or channel swapping, which can improve performance.\n",
    "\n",
    "- **Consistency in the OpenCV Ecosystem:**  \n",
    "  Over time, many OpenCV functions, tutorials, and sample codes have been built around BGR images. Changing the default now would break backward compatibility and require widespread refactoring.\n",
    "\n",
    "- **Interoperability:**  \n",
    "  Since OpenCV often interfaces with other libraries and legacy code that expect BGR ordering, it maintains BGR as a practical default.\n",
    "\n",
    "üî¢ **RGB - BGR Conversion (Channel Swap):**\n",
    "\n",
    "$$\n",
    "\\text{RGB ‚Üí BGR}: \n",
    "\\quad\n",
    "\\begin{bmatrix} B \\\\ G \\\\ R \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{BGR ‚Üí RGB}:\n",
    "\\quad\n",
    "\\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 1 & 0 & 0 \\end{bmatrix}\n",
    "\\begin{bmatrix} B \\\\ G \\\\ R \\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_2_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_BGR = np.array(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "BGR_TO_RGB = np.linalg.inv(RGB_TO_BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_bgr(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_BGR.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def bgr_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ BGR_TO_RGB.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_bgr = rgb_to_bgr(im_2)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"BGR Image\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_2_to_bgr] + [im_2_to_bgr[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_2_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_bgr = cv2.cvtColor(im_2, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"BGR Image\", \"Blue Channel\", \"Green Channel\", \"Red Channel\"]\n",
    "images = [im_2_to_bgr] + [im_2_to_bgr[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[RGB - YUV](#toc0_)\n",
    "\n",
    "A color space designed to separate luminance (brightness) information from chrominance (color) information.  \n",
    "\n",
    "- **Y** represents the luminance component (brightness).  \n",
    "- **U** and **V** represent the chrominance components (color information).\n",
    "\n",
    "üí° **Purpose of RGB to YUV Conversion:**\n",
    "\n",
    "- **Bandwidth Optimization in Video:**  \n",
    "  Since the human eye is more sensitive to brightness details than color details, YUV allows chrominance (U, V) to be subsampled, reducing the amount of color data transmitted or stored without a significant loss in perceived quality.\n",
    "\n",
    "- **Compression:**  \n",
    "  Many video and image compression standards (like MPEG, JPEG, and H.264) use YUV because it enables better compression efficiency by treating luminance and chrominance separately.\n",
    "\n",
    "- **Broadcast and Transmission:**  \n",
    "  YUV was originally developed for analog TV broadcasting to maintain backward compatibility with black-and-white televisions (which only needed the luminance signal).\n",
    "\n",
    "- **Color Manipulation:**  \n",
    "  Separating luminance and chrominance can simplify tasks like brightness adjustments and color filtering.\n",
    "\n",
    "üî¢ **RGB - YUV Conversion:**\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{RGB ‚Üí YUV:} \\quad\n",
    "& \\begin{bmatrix} Y \\\\ U \\\\ V \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0.299 & 0.587 & 0.114 \\\\\n",
    "-0.14713 & -0.28886 & 0.436 \\\\\n",
    "0.615 & -0.51499 & -0.10001\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} \\\\\n",
    "\\text{YUV ‚Üí RGB:} \\quad\n",
    "& \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1.13983 \\\\\n",
    "1 & -0.39465 & -0.58060 \\\\\n",
    "1 & 2.03211 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} Y \\\\ U \\\\ V \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_3_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_YUV = np.array(\n",
    "    [\n",
    "        [0.29900, 0.58700, 0.11400],\n",
    "        [-0.14713, -0.28886, 0.43600],\n",
    "        [0.61500, -0.51499, -0.10001],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "YUV_TO_RGB = np.linalg.inv(RGB_TO_YUV)\n",
    "YUV_OFFSET = np.array([0, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_yuv(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_YUV.T + YUV_OFFSET).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def yuv_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = ((image.reshape(-1, 3) - YUV_OFFSET) @ YUV_TO_RGB.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_yuv = rgb_to_yuv(im_2)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YUV Image\", \"Y Channel\", \"U Channel\", \"V Channel\"]\n",
    "images = [im_2_to_yuv] + [im_2_to_yuv[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_3_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_2_to_yuv = cv2.cvtColor(im_2, cv2.COLOR_RGB2YUV)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YUV Image\", \"Y Channel\", \"U Channel\", \"V Channel\"]\n",
    "images = [im_2_to_yuv] + [im_2_to_yuv[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_4_'></a>[RGB - YCbCr (YCC)](#toc0_)\n",
    "\n",
    "A color space used primarily in **digital video and image compression** (e.g., JPEG, MPEG, H.264), separating image luminance from chrominance components.\n",
    "\n",
    "- **Y** represents the **luma** component (brightness approximation).\n",
    "- **Cr** and **Cb** represent the **chrominance** components:\n",
    "  - **Cb** is the blue-difference chroma component.\n",
    "  - **Cr** is the red-difference chroma component.\n",
    "\n",
    "üí° **Purpose of RGB to Y'CrCb Conversion:**\n",
    "\n",
    "- **Digital Compression Efficiency:**  \n",
    "  Y'CrCb allows for **chroma subsampling** (like 4:2:0), significantly reducing data with minimal perceptual loss due to human visual system sensitivity.\n",
    "\n",
    "- **Standard in Digital Media:**  \n",
    "  Widely used in standards like **JPEG**, **MPEG**, and **broadcast video**, where it allows efficient encoding and transmission.\n",
    "\n",
    "- **Compatibility with Color TV Signals:**  \n",
    "  While derived from analog YUV, YCrCb is tailored for **digital representation**, including defined value ranges and offsets.\n",
    "\n",
    "üî¢ **RGB - Y'CrCb Conversion:**\n",
    "\n",
    "- **Full-range YCbCr (e.g., JPEG):**\n",
    "  - Y, Cb, and Cr all span the full 0‚Äì255 range.\n",
    "  - Common in image formats like **JPEG** and **PNG**.\n",
    "\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  \\text{RGB ‚Üí YCbCr:} \\quad\n",
    "  & \\begin{bmatrix} Y \\\\ Cb \\\\ Cr \\end{bmatrix} =\n",
    "  \\begin{bmatrix}\n",
    "  0.299 & 0.587 & 0.114 \\\\\n",
    "  -0.168736 & -0.331264 & 0.5 \\\\\n",
    "  0.5 & -0.418688 & -0.081312\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix}\n",
    "  +\n",
    "  \\begin{bmatrix} 0 \\\\ 128 \\\\ 128 \\end{bmatrix} \\\\\n",
    "  \\text{YCbCr ‚Üí RGB:} \\quad\n",
    "  & \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} =\n",
    "  \\begin{bmatrix}\n",
    "  1 & 0 & 1.402 \\\\\n",
    "  1 & -0.344136 & -0.714136 \\\\\n",
    "  1 & 1.772 & 0\n",
    "  \\end{bmatrix}\n",
    "  \\left(\n",
    "  \\begin{bmatrix} Y \\\\ Cb \\\\ Cr \\end{bmatrix} -\n",
    "  \\begin{bmatrix} 0 \\\\ 128 \\\\ 128 \\end{bmatrix}\n",
    "  \\right)\n",
    "  \\end{aligned}\n",
    "  $$\n",
    "\n",
    "- **Limited-range YCbCr (e.g., BT.601/BT.709):**\n",
    "  - Y is in the range **[16, 235]**, Cb/Cr in **[16, 240]**.\n",
    "  - Used in **broadcast video**, **MPEG**, **H.264**, etc.\n",
    "\n",
    "  $$\n",
    "  \\begin{aligned}\n",
    "  \\text{RGB ‚Üí YCbCr:} \\quad\n",
    "  & \\begin{bmatrix} Y \\\\ Cb \\\\ Cr \\end{bmatrix} =\n",
    "  \\begin{bmatrix}\n",
    "  0.257 & 0.504 & 0.098 \\\\\n",
    "  -0.148 & -0.291 & 0.439 \\\\\n",
    "  0.439 & -0.368 & -0.071\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix}\n",
    "  +\n",
    "  \\begin{bmatrix} 16 \\\\ 128 \\\\ 128 \\end{bmatrix} \\\\\n",
    "  \\text{YCbCr ‚Üí RGB:} \\quad\n",
    "  & \\begin{bmatrix} R \\\\ G \\\\ B \\end{bmatrix} =\n",
    "  \\begin{bmatrix}\n",
    "  1.164 & 0 & 1.596 \\\\\n",
    "  1.164 & -0.392 & -0.813 \\\\\n",
    "  1.164 & 2.017 & 0\n",
    "  \\end{bmatrix}\n",
    "  \\left(\n",
    "  \\begin{bmatrix} Y \\\\ Cb \\\\ Cr \\end{bmatrix} -\n",
    "  \\begin{bmatrix} 16 \\\\ 128 \\\\ 128 \\end{bmatrix}\n",
    "  \\right)\n",
    "  \\end{aligned}\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_4_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_YCBCR = np.array(\n",
    "    [\n",
    "        [0.299, 0.587, 0.114],\n",
    "        [-0.168736, -0.331264, 0.5],\n",
    "        [0.5, -0.418688, -0.081312],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "YCBCR_TO_RGB = np.linalg.inv(RGB_TO_YCBCR)\n",
    "YCBCR_OFFSET = np.array([0, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_ycbcr(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_YCBCR.T + YCBCR_OFFSET).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def ycbcr_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = ((image.reshape(-1, 3) - YCBCR_OFFSET) @ YCBCR_TO_RGB.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_3_to_ycrcb = rgb_to_ycbcr(im_3)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YCbCr Image\", \"Y Channel\", \"Cb Channel\", \"Cr Channel\"]\n",
    "images = [im_3_to_ycrcb] + [im_3_to_ycrcb[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_4_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_3_to_ycrcb = cv2.cvtColor(im_3, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YCbCr Image\", \"Y Channel\", \"Cb Channel\", \"Cr Channel\"]\n",
    "images = [im_3_to_ycrcb] + [im_3_to_ycrcb[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_5_'></a>[RGB - HSV](#toc0_)\n",
    "\n",
    "HSV stands for **Hue**, **Saturation**, and **Value** ‚Äî a cylindrical color space that separates image intensity (value) from color information (hue and saturation).  \n",
    "It is widely used in image processing, color pickers, and computer vision tasks.\n",
    "\n",
    "- **Hue (H):**  \n",
    "  Represents the color type (e.g., red, green, blue) as an angle in degrees on the color wheel.  \n",
    "  Range is typically [0¬∞, 360¬∞], but often scaled to [0, 1] or [0, 255] in digital systems.\n",
    "\n",
    "- **Saturation (S):**  \n",
    "  Measures the vibrancy or purity of the color (0 = grayscale, 1 = full color). Range: [0, 1] or [0, 255].\n",
    "\n",
    "- **Value (V):**  \n",
    "  Indicates brightness, where 0 is black and 1 is the brightest. Range: [0, 1] or [0, 255].\n",
    "\n",
    "üí° **Purpose of RGB to HSV Conversion:**\n",
    "\n",
    "- **Color Filtering and Detection:**  \n",
    "  HSV makes it easier to isolate colors because hue is decoupled from brightness.\n",
    "\n",
    "- **User Interfaces:**  \n",
    "  HSV is more intuitive for human interpretation and interaction, especially in tools like color pickers.\n",
    "\n",
    "- **Image Editing and Enhancement:**  \n",
    "  Brightness or saturation can be adjusted independently of hue, simplifying color manipulation.\n",
    "\n",
    "üî¢ **RGB to HSV Conversion (normalized RGB ‚àà [0,1]):**\n",
    "\n",
    "Let $R, G, B \\in [0, 1]$. First, compute:\n",
    "\n",
    "- $C_{\\text{max}} = \\max(R, G, B)$\n",
    "- $C_{\\text{min}} = \\min(R, G, B)$\n",
    "- $\\Delta = C_{\\text{max}} - C_{\\text{min}}$\n",
    "\n",
    "Then compute HSV components:\n",
    "\n",
    "$$\n",
    "V = C_{\\text{max}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = \\begin{cases}\n",
    "0 & \\text{if } C_{\\text{max}} = 0 \\\\\n",
    "\\frac{\\Delta}{C_{\\text{max}}} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\begin{cases}\n",
    "0 & \\text{if } \\Delta = 0 \\\\\n",
    "60^\\circ \\times \\left( \\frac{G - B}{\\Delta} \\mod 6 \\right) & \\text{if } C_{\\text{max}} = R \\\\\n",
    "60^\\circ \\times \\left( \\frac{B - R}{\\Delta} + 2 \\right) & \\text{if } C_{\\text{max}} = G \\\\\n",
    "60^\\circ \\times \\left( \\frac{R - G}{\\Delta} + 4 \\right) & \\text{if } C_{\\text{max}} = B\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**‚úçÔ∏è Note:**\n",
    "\n",
    "- The OpenCV implementation of HSV scales:\n",
    "  - **H to [0, 179]** instead of [0¬∞, 360¬∞]\n",
    "  - **S and V to [0, 255]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_5_1_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_3_to_hsv = cv2.cvtColor(im_3, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"HSV Image\", \"Hue Channel\", \"Saturation Channel\", \"Value Channel\"]\n",
    "images = [im_3_to_hsv] + [im_3_to_hsv[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_6_'></a>[RGB - CMYK](#toc0_)\n",
    "\n",
    "CMYK stands for **Cyan**, **Magenta**, **Yellow**, and **Key (Black)** ‚Äî a subtractive color model used primarily in color printing.  \n",
    "Unlike RGB (which is additive and designed for light-based displays), CMYK is based on ink or pigment absorption on physical media.\n",
    "\n",
    "- **C (Cyan):** Absorbs red light.\n",
    "- **M (Magenta):** Absorbs green light.\n",
    "- **Y (Yellow):** Absorbs blue light.\n",
    "- **K (Black):** Represents the black component used to enhance depth and reduce ink usage.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 40px; align-items: flex-start; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/RGB_combination_on_wall.png\"\n",
    "         alt=\"RGB_combination_on_wall.png\"\n",
    "         style=\"height: 500px; width: auto; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">\n",
    "      Additive Colors (¬©Ô∏è\n",
    "      <a href=\"https://en.wikipedia.org/wiki/Additive_color\">Wikipedia - Additive color</a>)\n",
    "    </div>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/SubtractiveColor.svg\"\n",
    "         alt=\"SubtractiveColor.svg\"\n",
    "         style=\"height: 500px; width: auto; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">\n",
    "      Subtractive Colors (¬©Ô∏è\n",
    "      <a href=\"https://en.wikipedia.org/wiki/Subtractive_color\">Wikipedia - Subtractive color</a>)\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "**Purpose of RGB to CMYK Conversion:**\n",
    "\n",
    "- **Printing:**  \n",
    "  CMYK is the standard color model for printers and press workflows, as it corresponds directly to ink usage.\n",
    "\n",
    "- **Color Separation:**  \n",
    "  The black (K) channel allows better shadow reproduction and text clarity without combining all three color inks.\n",
    "\n",
    "- **Cost Efficiency:**  \n",
    "  Using K instead of combining C, M, and Y for dark tones reduces ink consumption and drying time.\n",
    "\n",
    "üî¢ **RGB to CMYK Conversion (normalized RGB ‚àà [0,1]):**\n",
    "\n",
    "Let $R, G, B \\in [0, 1]$. First, compute the black component:\n",
    "\n",
    "$$\n",
    "K = 1 - \\max(R, G, B)\n",
    "$$\n",
    "\n",
    "Then compute the CMY components:\n",
    "\n",
    "$$\n",
    "C = \\frac{1 - R - K}{1 - K} \\quad\n",
    "M = \\frac{1 - G - K}{1 - K} \\quad\n",
    "Y = \\frac{1 - B - K}{1 - K}\n",
    "$$\n",
    "\n",
    "Special case:\n",
    "\n",
    "- If \\( R = G = B = 0 \\) (pure black), then \\( C = M = Y = 0 \\), \\( K = 1 \\)\n",
    "\n",
    "‚úçÔ∏è **Notes:**\n",
    "\n",
    "- CMYK is **device-dependent**, meaning values may vary depending on printer profiles and paper type.\n",
    "- RGB ‚Üí CMYK is **not reversible** in general, because CMYK has a smaller color gamut.\n",
    "- Normalized values are often scaled to \\([0, 100]\\%\\) or \\([0, 255]\\) for practical implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_6_1_'></a>[Using PIL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_3_to_cmyk = np.array(Image.fromarray(im_3).convert('CMYK'))\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4), layout=\"compressed\")\n",
    "titles = [\"CMYK Image\", \"Cyan\", \"Magenta\", \"Yellow\", \"Key\"]\n",
    "images = [Image.fromarray(im_3).convert('CMYK')] + [im_3_to_cmyk[:, :, i] for i in range(4)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Linear Color Transformations in RGB](#toc0_)\n",
    "\n",
    "Linear color transformations are applied directly to the RGB channels using **matrix operations or weighted sums**.  \n",
    "They preserve the **linearity of intensity relationships** between channels, which is useful for consistent color adjustment and image analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_1_'></a>[Warming Filters](#toc0_)\n",
    "\n",
    "Warming filters shift colors toward **reds, oranges, and yellows**, giving images a warmer, sunlit feel.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_1_'></a>[Sepia](#toc0_)\n",
    "\n",
    "Sepia is a classic warming filter that gives images a **brownish, antique look**, simulating old photographs.  \n",
    "Sepia is a **one-way transformation**; it cannot be perfectly inverted to recover the original RGB image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEPIA_MATRIX = np.array(\n",
    "    [\n",
    "        [0.393, 0.769, 0.189],  # R channel: strong green, moderate red, little blue\n",
    "        [0.349, 0.686, 0.168],  # G channel: strong green, moderate red, little blue\n",
    "        [0.227, 0.534, 0.131],  # B channel: strong green, moderate red, little blue\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "im_3_sepia = cv2.transform(im_3, SEPIA_MATRIX)  # (image.reshape(-1, 3) @ SEPIA_MATRIX.T).reshape(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Sepia Image\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_3_sepia, im_3_sepia[:, :, 0], im_3_sepia[:, :, 1], im_3_sepia[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_2_'></a>[Rosewood](#toc0_)\n",
    "\n",
    "Rosewood gives images a **warm reddish-brown tint**, adding depth and richness.  \n",
    "It is a **one-way transformation**; original colors cannot be perfectly recovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROSEWOOD_MATRIX = np.array(\n",
    "    [\n",
    "        [0.5, 0.25, 0.5],  # R channel: strong red, some green, strong blue\n",
    "        [0.3, 0.6, 0.1],   # G channel: moderate red, strong green, little blue\n",
    "        [0.4, 0.3, 0.5],   # B channel: strong red, some green, strong blue\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "im_3_rosewood = cv2.transform(im_3, ROSEWOOD_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Rosewood Image\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_3_rosewood, im_3_rosewood[:, :, 0], im_3_rosewood[:, :, 1], im_3_rosewood[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_3_'></a>[Amber / Golden](#toc0_)\n",
    "\n",
    "Amber/Golden adds a **soft golden-yellow glow**, enhancing warmth and highlights.  \n",
    "It is a **one-way transformation**; the original RGB cannot be restored exactly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMBER_MATRIX = np.array(\n",
    "    [\n",
    "        [0.6, 0.4, 0.1],  # R channel: strong red, some green, little blue\n",
    "        [0.3, 0.7, 0.2],  # G channel: moderate red, strong green, some blue\n",
    "        [0.1, 0.2, 0.6],  # B channel: small red, some green, dominant blue\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "im_3_amber = cv2.transform(im_3, AMBER_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Amber Image\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_3_amber, im_3_amber[:, :, 0], im_3_amber[:, :, 1], im_3_amber[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_1_4_'></a>[Sunset / Autumn](#toc0_)\n",
    "\n",
    "Sunset/Autumn gives a **warm orange-red tone**, evoking sunsets or autumn leaves.  \n",
    "It is a **one-way transformation**; cannot revert to the original image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUNSET_MATRIX = np.array(\n",
    "    [\n",
    "        [0.9, 0.5, 0.2],  # R channel: strong red, some green, little blue\n",
    "        [0.4, 0.6, 0.1],  # G channel: moderate red, strong green, minimal blue\n",
    "        [0.2, 0.3, 0.5],  # B channel: small red, some green, dominant blue\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "im_3_sunset = cv2.transform(im_3, SUNSET_MATRIX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Sunset Image\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_3_sunset, im_3_sunset[:, :, 0], im_3_sunset[:, :, 1], im_3_sunset[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_2_'></a>[Cooling Filters](#toc0_)\n",
    "\n",
    "Cooling filters shift colors toward **blues, cyans, and greens**, giving images a cooler, calm, or nighttime feel.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_2_1_'></a>[Cool Blue](#toc0_)\n",
    "\n",
    "Cool Blue gives a **cool blue tone**, emphasizing blues while reducing reds and greens.  \n",
    "It creates a **calming or icy atmosphere**, often used to simulate evening light or cold scenes.  \n",
    "It is a **one-way transformation**; the original colors cannot be perfectly recovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COOL_BLUE_MATRIX = np.array(\n",
    "    [\n",
    "        [0.2, 0.3, 0.7],  # R channel: less red, more blue\n",
    "        [0.1, 0.6, 0.4],  # G channel: balanced\n",
    "        [0.0, 0.2, 0.8],  # B channel: strong blue\n",
    "    ],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "im_2_coolblue = cv2.transform(im_2.astype(np.float32), COOL_BLUE_MATRIX)\n",
    "im_2_coolblue = np.clip(im_2_coolblue, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Cool Blue\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_2_coolblue, im_2_coolblue[:, :, 0], im_2_coolblue[:, :, 1], im_2_coolblue[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_2_2_'></a>[Cyan Tint](#toc0_)\n",
    "\n",
    "Cyan Tint gives a **soft cyan-blue tone**, enhancing blues and greens while reducing reds.  \n",
    "It produces a **cool, refreshing effect**, often used to simulate water, glass, or futuristic lighting.  \n",
    "It is a **one-way transformation**; the original colors cannot be perfectly recovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CYAN_MATRIX = np.array(\n",
    "    [\n",
    "        [0.1, 0.5, 0.6],  # R channel: reduced red\n",
    "        [0.2, 0.7, 0.5],  # G channel: green + blue emphasis\n",
    "        [0.3, 0.4, 0.8],  # B channel: strong blue\n",
    "    ],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "im_2_cyan = cv2.transform(im_2.astype(np.float32), CYAN_MATRIX)\n",
    "im_2_cyan = np.clip(im_2_cyan, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Cyan Tint\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_2_cyan, im_2_cyan[:, :, 0], im_2_cyan[:, :, 1], im_2_cyan[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_3_2_3_'></a>[Moonlight](#toc0_)\n",
    "\n",
    "Moonlight gives a **pale bluish tone**, emphasizing cool shades and softening warm colors.  \n",
    "It creates a **nighttime or mystical atmosphere**, simulating moonlit scenes.  \n",
    "It is a **one-way transformation**; the original colors cannot be perfectly recovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MOONLIGHT_MATRIX = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, 0.6],  # R channel: very low red\n",
    "        [0.1, 0.5, 0.5],  # G channel: muted green\n",
    "        [0.2, 0.3, 0.7],  # B channel: dominant blue\n",
    "    ],\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "im_2_moonlight = cv2.transform(im_2.astype(np.float32), MOONLIGHT_MATRIX)\n",
    "im_2_moonlight = np.clip(im_2_moonlight, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Moonlight\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_2_moonlight, im_2_moonlight[:, :, 0], im_2_moonlight[:, :, 1], im_2_moonlight[:, :, 2]]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_4_'></a>[Nonlinear Color and Photographic Effects](#toc0_)\n",
    "\n",
    "Nonlinear color transformations modify pixel intensities using **nonlinear functions or mapping**, rather than simple linear matrices.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_1_'></a>[Monochromatic Filters](#toc0_)\n",
    "\n",
    "Monochromatic filters map an image to **shades of a single color** or grayscale while preserving relative intensity.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_4_1_1_'></a>[Duotone](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duotone(image: NDArray, color_dark: tuple[int, int, int], color_light: tuple[int, int, int]) -> NDArray:\n",
    "\n",
    "    # convert to float\n",
    "    img = image.astype(np.float32)\n",
    "\n",
    "    # luminance (perceptual grayscale)\n",
    "    gs = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gs = cv2.normalize(gs, None, 0, 1, cv2.NORM_MINMAX)  # normalize to [0, 1]\n",
    "\n",
    "    # prepare colors\n",
    "    c0 = np.array(color_dark, dtype=np.float32)\n",
    "    c1 = np.array(color_light, dtype=np.float32)\n",
    "\n",
    "    # nonlinear interpolation\n",
    "    duotone_img = (1 - gs[..., None]) * c0 + gs[..., None] * c1\n",
    "\n",
    "    return np.clip(duotone_img, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duotone_1 = duotone(im_3, color_dark=(20, 30, 100), color_light=(150, 30, 20))\n",
    "duotone_2 = duotone(im_3, color_dark=(40, 30, 100), color_light=(0, 200, 0))\n",
    "duotone_3 = duotone(im_3, color_dark=(50, 200, 30), color_light=(150, 0, 150))\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"Duotone 1\", \"Duotone 2\", \"Duotone 3\"]\n",
    "images = [im_3, duotone_1, duotone_2, duotone_3]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_4_1_2_'></a>[Tritone](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tritone(\n",
    "    image: NDArray,\n",
    "    color_shadow: tuple[int, int, int],\n",
    "    color_mid: tuple[int, int, int],\n",
    "    color_high: tuple[int, int, int],\n",
    ") -> NDArray:\n",
    "\n",
    "    # convert to float\n",
    "    img = image.astype(np.float32)\n",
    "\n",
    "    # luminance (perceptual grayscale)\n",
    "    gs = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    gs = cv2.normalize(gs, None, 0, 1, cv2.NORM_MINMAX)  # normalize to [0, 1]\n",
    "\n",
    "    # prepare colors\n",
    "    c0 = np.array(color_shadow, dtype=np.float32)\n",
    "    c1 = np.array(color_mid, dtype=np.float32)\n",
    "    c2 = np.array(color_high, dtype=np.float32)\n",
    "\n",
    "    out = np.zeros_like(img)\n",
    "\n",
    "    # shadow to midtones\n",
    "    mask_low = gs <= 0.5\n",
    "    t = gs[mask_low] * 2.0\n",
    "    out[mask_low] = (1 - t[:, None]) * c0 + t[:, None] * c1\n",
    "\n",
    "    # midtones to highlight\n",
    "    mask_high = gs > 0.5\n",
    "    t = (gs[mask_high] - 0.5) * 2.0\n",
    "    out[mask_high] = (1 - t[:, None]) * c1 + t[:, None] * c2\n",
    "\n",
    "    return np.clip(out, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tritone_1 = tritone(\n",
    "    im_3,\n",
    "    color_shadow=(20, 30, 120),  # deep blue shadows\n",
    "    color_mid=(180, 160, 80),    # warm midtones\n",
    "    color_high=(250, 240, 220),  # soft highlights\n",
    ")\n",
    "tritone_2 = tritone(\n",
    "    im_3,\n",
    "    color_shadow=(60, 40, 20),   # dark brown\n",
    "    color_mid=(160, 120, 80),    # sepia midtones\n",
    "    color_high=(240, 220, 180),  # parchment highlights\n",
    ")\n",
    "tritone_3 = tritone(\n",
    "    im_3,\n",
    "    color_shadow=(0, 0, 0),      # black\n",
    "    color_mid=(120, 120, 120),   # neutral gray\n",
    "    color_high=(255, 255, 255),  # white\n",
    ")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"Tritone 1\", \"Tritone 2\", \"Tritone 3\"]\n",
    "images = [im_3, tritone_1, tritone_2, tritone_3]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_4_2_'></a>[Photographic / Special Effects](#toc0_)\n",
    "\n",
    "Photographic or special effects apply **creative, nonlinear transformations** to RGB values for visual impact.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_4_2_1_'></a>[Solarize](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solarize_1 = ImageOps.solarize(image_3, threshold=64)\n",
    "solarize_2 = ImageOps.solarize(image_3, threshold=128)\n",
    "solarize_3 = ImageOps.solarize(image_3, threshold=192)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"Solarize 1\", \"Solarize 2\", \"Solarize 3\"]\n",
    "images = [im_3, solarize_1, solarize_2, solarize_3]\n",
    "\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_5_'></a>[Visualize using Custom Color Maps](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_5_1_'></a>[Indexed Image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch color palletes\n",
    "colors = np.array(image_1.getpalette(), dtype=np.uint8).reshape(-1, 3)\n",
    "num_colors =colors.shape[0]\n",
    "\n",
    "# find alpha index\n",
    "alpha_idx = np.ones(num_colors)\n",
    "if 'transparency' in image_1.info:\n",
    "    t = image_1.info['transparency']\n",
    "    if isinstance(t, bytes):\n",
    "        alpha_idx = np.array(list(t), dtype=np.uint8) / 255.0\n",
    "    elif isinstance(t, int):\n",
    "        alpha_idx[t] = 0.0\n",
    "\n",
    "# add alpha channel + normalization\n",
    "colors_norm = np.concatenate([colors / 255.0, alpha_idx[:, None]], axis=1)\n",
    "\n",
    "# log\n",
    "print(f\"num colors : {num_colors}\")\n",
    "print(f\"alpha idx  : {alpha_idx}\")\n",
    "print(f\"colors:\\n{colors_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a color map\n",
    "cmap = ListedColormap(colors_norm)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), layout=\"compressed\")\n",
    "axs[0].imshow(im_1, vmin=0, vmax=im_1.max())\n",
    "axs[0].set_title(\"Wrong cmap\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(im_1, cmap=cmap, vmin=0, vmax=num_colors)\n",
    "axs[1].set_title(\"Correct cmap\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "media-processing-workshop (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  },
  "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
