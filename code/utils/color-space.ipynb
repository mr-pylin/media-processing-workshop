{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4;\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/media-processing-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"display: flex; justify-content: flex-end; flex: 1; gap: 8px; align-items: center; padding: 0;\">\n",
    "        <a href=\"https://opencv.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/opencv/logo/OpenCV_logo_no_text-1.svg\"\n",
    "                 alt=\"OpenCV Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://pillow.readthedocs.io/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/pillow/logo/pillow-logo-248x250.png\"\n",
    "                 alt=\"PIL Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scikit-image.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/scikit-image/logo/logo.png\"\n",
    "                 alt=\"scikit-image Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scipy.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/scipy/logo/logo.svg\"\n",
    "                 alt=\"SciPy Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Color Theory Fundamentals](#toc1_)    \n",
    "  - [Human Vision Basics](#toc1_1_)    \n",
    "  - [Human Color Gamut](#toc1_2_)    \n",
    "- [Dependencies](#toc2_)    \n",
    "- [Load Images](#toc3_)    \n",
    "- [Color Space Conversion](#toc4_)    \n",
    "  - [RGB - Grayscale](#toc4_1_)    \n",
    "    - [Manual](#toc4_1_1_)    \n",
    "    - [Using OpenCV](#toc4_1_2_)    \n",
    "    - [Using PIL](#toc4_1_3_)    \n",
    "    - [Using Scikit-Image](#toc4_1_4_)    \n",
    "  - [RGB - BGR](#toc4_2_)    \n",
    "    - [Manual](#toc4_2_1_)    \n",
    "    - [Using OpenCV](#toc4_2_2_)    \n",
    "  - [RGB - YUV](#toc4_3_)    \n",
    "    - [Manual](#toc4_3_1_)    \n",
    "    - [Using OpenCV](#toc4_3_2_)    \n",
    "  - [RGB - YCbCr (YCC)](#toc4_4_)    \n",
    "    - [Manual](#toc4_4_1_)    \n",
    "    - [Using OpenCV](#toc4_4_2_)    \n",
    "  - [RGB - HSV](#toc4_5_)    \n",
    "    - [Using OpenCV](#toc4_5_1_)    \n",
    "  - [RGB - CMYK](#toc4_6_)    \n",
    "    - [Using PIL](#toc4_6_1_)    \n",
    "- [Visualize using Custom Color Maps](#toc5_)    \n",
    "  - [Indexed Image](#toc5_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Color Theory Fundamentals](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Human Vision Basics](#toc0_)\n",
    "\n",
    "- **Spectral range**: 380-700nm  \n",
    "- **Color discrimination**: ~10-11 million colors (optimal conditions)  \n",
    "- **Cone types**: S (420nm), M (530nm), L (560nm) peak sensitivity\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 40px; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/Cone-fundamentals-with-srgb-spectrum.svg\"\n",
    "         alt=\"Cone-fundamentals-with-srgb-spectrum.svg\"\n",
    "         style=\"max-height: 250px; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">Normalized responsivity spectra of human cone cells, S, M, and L types</div>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/ConeMosaics.png\"\n",
    "         alt=\"ConeMosaics.png\"\n",
    "         style=\"max-height: 250px; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">Distribution of cone cells in the fovea of an individual with normal color vision (left), and a color blind (protanopic) retina</div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; margin-top: 15px;\">\n",
    "  Visual representation of cone cell characteristics and distribution (¬©Ô∏è \n",
    "  <a href=\"https://en.wikipedia.org/wiki/Cone_cell\">Wikipedia - Cone cell</a>)\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Human Color Gamut](#toc0_)\n",
    "\n",
    "The **human color gamut** represents all colors perceivable by human vision, visualized through the CIE 1931 chromaticity diagram.  \n",
    "Approximately ~8% of males and ~0.5% of females are affected by color blindness, which impacts their perception of this gamut.\n",
    "\n",
    "<figure style=\"text-align: center; margin: 0;\">\n",
    "  <div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <div style=\"flex: 1; background-color: lightgray; padding: 10px; display: flex; align-items: center; justify-content: center;\">\n",
    "      <img src=\"../../assets/images/third_party/Rechteckspektrum_sRGB.svg\"\n",
    "           alt=\"Rechteckspektrum_sRGB.svg\"\n",
    "           style=\"max-width: 100%; height: auto;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; background-color: lightgray; padding: 10px; display: flex; align-items: center; justify-content: center;\">\n",
    "      <img src=\"../../assets/images/third_party/Cie_Chart_with_sRGB_gamut_by_spigget.png\"\n",
    "           alt=\"Cie_Chart_with_sRGB_gamut_by_spigget.png\"\n",
    "           style=\"max-width: 80%; height: auto;\">\n",
    "    </div>\n",
    "    <div style=\"flex: 1; background-color: lightgray; padding: 10px; display: flex; align-items: center; justify-content: center;\">\n",
    "      <img src=\"../../assets/images/third_party/CIE1931xy_gamut_comparison.svg\"\n",
    "           alt=\"CIE1931xy_gamut_comparison.svg\"\n",
    "           style=\"max-width: 90%; height: auto;\">\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Visual representations of color gamuts in sRGB space (¬©Ô∏è \n",
    "    <a href=\"https://en.wikipedia.org/wiki/Gamut\">Wikipedia - Gamut</a>)\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "**CIE Coordinates (Simplified DIP Approach):**\n",
    "\n",
    "In digital image processing, chromaticity coordinates can be simplified as:\n",
    "\n",
    "- **x**: Red component\n",
    "- **y**: Green component  \n",
    "- **z**: Blue component (z = 1-x-y)\n",
    "\n",
    "**Example**: 780nm wavelength ‚Üí x=0.74, y=0.26, z=0.00 (deep red)\n",
    "\n",
    "**RGB Display Limitations:**\n",
    "\n",
    "Modern displays reproduce only a subset of human-visible colors:\n",
    "\n",
    "- **sRGB**: ~35% coverage\n",
    "- **Adobe RGB**: ~50% coverage\n",
    "- **DCI-P3**: ~45% coverage\n",
    "\n",
    "**2D vs 3D Color Space:**\n",
    "\n",
    "- **2D diagram**: Chromaticity (hue/saturation) at constant luminance\n",
    "- **3D space**: Adds brightness dimension (Y-value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Dependencies](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage as ski\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy.typing import NDArray\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Load Images](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1 = Image.open(fp=\"../../assets/images/dip_3rd/CH06_Fig0638(a)(lenna_RGB).tif\")\n",
    "image_2 = Image.open(fp=\"../../assets/images/misc/lenna_rgba_indexed.png\")\n",
    "\n",
    "# PIL.Image.Image to np.ndarray\n",
    "im_1 = np.array(image_1)\n",
    "im_2 = np.array(image_2)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), layout=\"compressed\")\n",
    "axs[0].imshow(image_1, vmin=0, vmax=255)\n",
    "axs[0].set_title(\"RGB\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(image_2, vmin=0, vmax=255)\n",
    "axs[1].set_title(\"Indexed RGBA\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_1, im_1[:, :, 0], im_1[:, :, 1], im_1[:, :, 2]]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if len(img.shape) == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Color Space Conversion](#toc0_)\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `cv2.cvtColor`: [docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#gaf86c09fe702ed037c03c2bc603ceab14](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html#gaf86c09fe702ed037c03c2bc603ceab14)\n",
    "- Color Space Conversions [`cv2`]: [docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html](https://docs.opencv.org/master/d8/d01/group__imgproc__color__conversions.html)\n",
    "- Modes [`PIL`]: [pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes](https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes)\n",
    "- Colormap reference [`matplotlib`]: [matplotlib.org/stable/gallery/color/colormap_reference.html](https://matplotlib.org/stable/gallery/color/colormap_reference.html)\n",
    "- ITU-R Recommendation BT.601-7 [an international technical standard]: [itu.int/dms_pubrec/itu-r/rec/bt/r-rec-bt.601-7-201103-i!!pdf-e.pdf](http://itu.int/dms_pubrec/itu-r/rec/bt/r-rec-bt.601-7-201103-i!!pdf-e.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[RGB - Grayscale](#toc0_)\n",
    "\n",
    "Converting an RGB image to grayscale simplifies the color information by reducing colors into shades of gray.   \n",
    "This process serves several important purposes:\n",
    "\n",
    "- **Data Reduction:**  \n",
    "  Grayscale images require less memory and computational power than full-color images since they contain only one channel instead of three.\n",
    "\n",
    "- **Preprocessing for Computer Vision:**  \n",
    "  Many image processing and computer vision algorithms (e.g., edge detection, segmentation, pattern recognition) operate more efficiently or exclusively on grayscale images, as color information may be redundant or irrelevant.\n",
    "\n",
    "- **Highlighting Intensity Information:**  \n",
    "  Grayscale focuses solely on luminance or brightness variations, which often correspond better to structural details in images compared to color information.\n",
    "\n",
    "- **Simplifying Analysis:**  \n",
    "  In fields like medical imaging, document scanning, or texture analysis, grayscale images help isolate texture, shape, and contrast without the distraction of color.\n",
    "\n",
    "üî¢ **RGB to Grayscale Conversion Methods:**\n",
    "\n",
    "1. **Luminance (perceptual weights, standard method):**  \n",
    "   Human vision is more sensitive to green than to red and blue. The weighted sum is used in most image processing libraries:\n",
    "\n",
    "   $$\n",
    "   Y = 0.299 \\, R + 0.587 \\, G + 0.114 \\, B\n",
    "   $$\n",
    "\n",
    "   ‚úÖ **Use when:** Accurate brightness representation is needed, e.g., for human viewing, compression, or perceptual tasks.\n",
    "\n",
    "2. **Averaging Method:**  \n",
    "   Simply take the arithmetic mean of the three channels:\n",
    "\n",
    "   $$\n",
    "   Y = \\frac{R + G + B}{3}\n",
    "   $$\n",
    "\n",
    "   ‚úÖ **Use when:** Simplicity is more important than perceptual accuracy, e.g., quick visualization, basic preprocessing.\n",
    "\n",
    "3. **L2 Norm (Euclidean norm method):**  \n",
    "   Computes the magnitude of the RGB vector:\n",
    "\n",
    "   $$\n",
    "   Y = \\sqrt{R^2 + G^2 + B^2 \\over 3}\n",
    "   $$\n",
    "\n",
    "   ‚úÖ **Use when:** Emphasizing overall intensity magnitude or when treating RGB as a vector in feature extraction or scientific imaging.\n",
    "\n",
    "4. **Max/Min Channel Method:**  \n",
    "   Take the maximum or minimum of R, G, B as the grayscale value:\n",
    "\n",
    "   $$\n",
    "   Y = \\max(R, G, B) \\quad \\text{or} \\quad Y = \\min(R, G, B)\n",
    "   $$\n",
    "\n",
    "   ‚úÖ **Use when:** Highlighting the strongest or weakest channel for edge detection or contrast emphasis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_GS = np.array(\n",
    "    [\n",
    "        [0.299, 0.587, 0.114],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "# approximation using pseudo-inverse [best linear least-squares approximation]\n",
    "GS_TO_RGB = np.linalg.pinv(RGB_TO_GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_gs(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_GS.T).reshape(image.shape[:2])\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def gs_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    rgb = (image.reshape(-1, 1) @ GS_TO_RGB.T).reshape(*image.shape, 3)\n",
    "    return np.clip(rgb, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_gs_1  = rgb_to_gs(im_1)\n",
    "gs_1_to_rgb_1 = gs_to_rgb(im_1_to_gs_1)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (data loss)\"]\n",
    "images = [im_1, im_1_to_gs_1, gs_1_to_rgb_1]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_gs_2 = cv2.cvtColor(im_1, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# replicates the grayscale channel into all three RGB channels (still grayish image)\n",
    "gs_2_to_rgb_2 = cv2.cvtColor(im_1_to_gs_2, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (data loss)\"]\n",
    "images = [im_1, im_1_to_gs_2, gs_2_to_rgb_2]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_3_'></a>[Using PIL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_gs_3 = Image.fromarray(im_1).convert(\"L\")\n",
    "gs_3_to_rgb_3 = im_1_to_gs_3.convert(\"RGB\")\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (data loss)\"]\n",
    "images = [im_1, im_1_to_gs_3, gs_3_to_rgb_3]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_1_4_'></a>[Using Scikit-Image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_gs_4 = ski.color.rgb2gray(im_1)\n",
    "gs_4_to_rgb_4 = ski.color.gray2rgb(im_1_to_gs_4)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4), layout=\"compressed\")\n",
    "titles = [\"Original\", \"RGB to GS\", \"GS to RGB (data loss)\"]\n",
    "images = [im_1, im_1_to_gs_4, gs_4_to_rgb_4]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[RGB - BGR](#toc0_)\n",
    "\n",
    "OpenCV uses the BGR (Blue, Green, Red) channel order as its default color format instead of the more common RGB order for historical and compatibility reasons:\n",
    "\n",
    "- **Historical Legacy:**  \n",
    "  Early versions of OpenCV were designed to be compatible with the Windows bitmap (BMP) image format, which stores pixel data in BGR order. Using BGR natively simplified reading and writing BMP files without additional channel reordering.\n",
    "\n",
    "- **Performance Considerations:**  \n",
    "  By matching the underlying image formats and memory layouts commonly used on Windows systems, OpenCV avoids unnecessary data copying or channel swapping, which can improve performance.\n",
    "\n",
    "- **Consistency in the OpenCV Ecosystem:**  \n",
    "  Over time, many OpenCV functions, tutorials, and sample codes have been built around BGR images. Changing the default now would break backward compatibility and require widespread refactoring.\n",
    "\n",
    "- **Interoperability:**  \n",
    "  Since OpenCV often interfaces with other libraries and legacy code that expect BGR ordering, it maintains BGR as a practical default.\n",
    "\n",
    "üî¢ **RGB to BGR Conversion (Channel Swap):**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "B \\\\ G \\\\ R\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "R \\\\ G \\\\ B\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_BGR = np.array(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "BGR_TO_RGB = np.linalg.inv(RGB_TO_BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_bgr(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_BGR.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def bgr_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ BGR_TO_RGB.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_bgr = rgb_to_bgr(im_1)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"BGR Image\", \"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "images = [im_1_to_bgr] + [im_1_to_bgr[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_2_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_bgr = cv2.cvtColor(im_1, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"BGR Image\", \"Blue Channel\", \"Green Channel\", \"Red Channel\"]\n",
    "images = [im_1_to_bgr] + [im_1_to_bgr[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[RGB - YUV](#toc0_)\n",
    "\n",
    "A color space designed to separate luminance (brightness) information from chrominance (color) information.  \n",
    "\n",
    "- **Y** represents the luminance component (brightness).  \n",
    "- **U** and **V** represent the chrominance components (color information).\n",
    "\n",
    "**Purpose of RGB to YUV Conversion:**\n",
    "\n",
    "- **Bandwidth Optimization in Video:**  \n",
    "  Since the human eye is more sensitive to brightness details than color details, YUV allows chrominance (U, V) to be subsampled, reducing the amount of color data transmitted or stored without a significant loss in perceived quality.\n",
    "\n",
    "- **Compression:**  \n",
    "  Many video and image compression standards (like MPEG, JPEG, and H.264) use YUV because it enables better compression efficiency by treating luminance and chrominance separately.\n",
    "\n",
    "- **Broadcast and Transmission:**  \n",
    "  YUV was originally developed for analog TV broadcasting to maintain backward compatibility with black-and-white televisions (which only needed the luminance signal).\n",
    "\n",
    "- **Color Manipulation:**  \n",
    "  Separating luminance and chrominance can simplify tasks like brightness adjustments and color filtering.\n",
    "\n",
    "üî¢ **RGB to YUV Conversion:**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Y \\\\\n",
    "U \\\\\n",
    "V\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.299 & 0.587 & 0.114 \\\\\n",
    "-0.14713 & -0.28886 & 0.436 \\\\\n",
    "0.615 & -0.51499 & -0.10001\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "R \\\\\n",
    "G \\\\\n",
    "B\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_YUV = np.array(\n",
    "    [\n",
    "        [0.29900, 0.58700, 0.11400],\n",
    "        [-0.14713, -0.28886, 0.43600],\n",
    "        [0.61500, -0.51499, -0.10001],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "YUV_TO_RGB = np.linalg.inv(RGB_TO_YUV)\n",
    "YUV_OFFSET = np.array([0, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_yuv(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_YUV.T + YUV_OFFSET).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def yuv_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = ((image.reshape(-1, 3) - YUV_OFFSET) @ YUV_TO_RGB.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_yuv = rgb_to_yuv(im_1)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YUV Image\", \"Y Channel\", \"U Channel\", \"V Channel\"]\n",
    "images = [im_1_to_yuv] + [im_1_to_yuv[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_3_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_yuv = cv2.cvtColor(im_1, cv2.COLOR_RGB2YUV)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YUV Image\", \"Y Channel\", \"U Channel\", \"V Channel\"]\n",
    "images = [im_1_to_yuv] + [im_1_to_yuv[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[RGB - YCbCr (YCC)](#toc0_)\n",
    "\n",
    "A color space used primarily in **digital video and image compression** (e.g., JPEG, MPEG, H.264), separating image luminance from chrominance components.\n",
    "\n",
    "- **Y** represents the **luma** component (brightness approximation).\n",
    "- **Cr** and **Cb** represent the **chrominance** components:\n",
    "  - **Cb** is the blue-difference chroma component.\n",
    "  - **Cr** is the red-difference chroma component.\n",
    "\n",
    "**Purpose of RGB to Y'CrCb Conversion:**\n",
    "\n",
    "- **Digital Compression Efficiency:**  \n",
    "  Y'CrCb allows for **chroma subsampling** (like 4:2:0), significantly reducing data with minimal perceptual loss due to human visual system sensitivity.\n",
    "\n",
    "- **Standard in Digital Media:**  \n",
    "  Widely used in standards like **JPEG**, **MPEG**, and **broadcast video**, where it allows efficient encoding and transmission.\n",
    "\n",
    "- **Compatibility with Color TV Signals:**  \n",
    "  While derived from analog YUV, YCrCb is tailored for **digital representation**, including defined value ranges and offsets.\n",
    "\n",
    "üî¢ **RGB to Y'CrCb Conversion (BT.601 Standard):**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "Y \\\\\n",
    "Cb \\\\\n",
    "Cr\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.299 & 0.587 & 0.114 \\\\\n",
    "-0.168736 & -0.331264 & 0.5 \\\\\n",
    "0.5 & -0.418688 & -0.081312\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "R \\\\\n",
    "G \\\\\n",
    "B\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "128 \\\\\n",
    "128\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**‚úçÔ∏è Notes:**\n",
    "\n",
    "There are multiple YCrCb (YCbCr) variants depending on usage:\n",
    "\n",
    "- **Full-range YCbCr (e.g., JPEG):**\n",
    "  - Y, Cb, and Cr all span the full 0‚Äì255 range.\n",
    "  - Common in image formats like **JPEG** and **PNG**.\n",
    "\n",
    "- **Limited-range YCbCr (e.g., BT.601/BT.709):**\n",
    "  - Y is in the range **[16, 235]**, Cb/Cr in **[16, 240]**.\n",
    "  - Used in **broadcast video**, **MPEG**, **H.264**, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGB_TO_YCBCR = np.array(\n",
    "    [\n",
    "        [0.299, 0.587, 0.114],\n",
    "        [-0.168736, -0.331264, 0.5],\n",
    "        [0.5, -0.418688, -0.081312],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "YCBCR_TO_RGB = np.linalg.inv(RGB_TO_YCBCR)\n",
    "YCBCR_OFFSET = np.array([0, 128, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_ycbcr(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = (image.reshape(-1, 3) @ RGB_TO_YCBCR.T + YCBCR_OFFSET).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def ycbcr_to_rgb(image: NDArray) -> NDArray[np.uint8]:\n",
    "    im = ((image.reshape(-1, 3) - YCBCR_OFFSET) @ YCBCR_TO_RGB.T).reshape(image.shape)\n",
    "    return np.clip(im, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_ycrcb = rgb_to_ycbcr(im_1)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YCbCr Image\", \"Y Channel\", \"Cb Channel\", \"Cr Channel\"]\n",
    "images = [im_1_to_ycrcb] + [im_1_to_ycrcb[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_4_2_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_ycrcb = cv2.cvtColor(im_1, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"YCbCr Image\", \"Y Channel\", \"Cb Channel\", \"Cr Channel\"]\n",
    "images = [im_1_to_ycrcb] + [im_1_to_ycrcb[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_5_'></a>[RGB - HSV](#toc0_)\n",
    "\n",
    "HSV stands for **Hue**, **Saturation**, and **Value** ‚Äî a cylindrical color space that separates image intensity (value) from color information (hue and saturation).  \n",
    "It is widely used in image processing, color pickers, and computer vision tasks.\n",
    "\n",
    "- **Hue (H):**  \n",
    "  Represents the color type (e.g., red, green, blue) as an angle in degrees on the color wheel.  \n",
    "  Range is typically [0¬∞, 360¬∞], but often scaled to [0, 1] or [0, 255] in digital systems.\n",
    "\n",
    "- **Saturation (S):**  \n",
    "  Measures the vibrancy or purity of the color (0 = grayscale, 1 = full color). Range: [0, 1] or [0, 255].\n",
    "\n",
    "- **Value (V):**  \n",
    "  Indicates brightness, where 0 is black and 1 is the brightest. Range: [0, 1] or [0, 255].\n",
    "\n",
    "**Purpose of RGB to HSV Conversion:**\n",
    "\n",
    "- **Color Filtering and Detection:**  \n",
    "  HSV makes it easier to isolate colors because hue is decoupled from brightness.\n",
    "\n",
    "- **User Interfaces:**  \n",
    "  HSV is more intuitive for human interpretation and interaction, especially in tools like color pickers.\n",
    "\n",
    "- **Image Editing and Enhancement:**  \n",
    "  Brightness or saturation can be adjusted independently of hue, simplifying color manipulation.\n",
    "\n",
    "üî¢ **RGB to HSV Conversion (normalized RGB ‚àà [0,1]):**\n",
    "\n",
    "Let $R, G, B \\in [0, 1]$. First, compute:\n",
    "\n",
    "- $C_{\\text{max}} = \\max(R, G, B)$\n",
    "- $C_{\\text{min}} = \\min(R, G, B)$\n",
    "- $\\Delta = C_{\\text{max}} - C_{\\text{min}}$\n",
    "\n",
    "Then compute HSV components:\n",
    "\n",
    "$$\n",
    "V = C_{\\text{max}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "S = \\begin{cases}\n",
    "0 & \\text{if } C_{\\text{max}} = 0 \\\\\n",
    "\\frac{\\Delta}{C_{\\text{max}}} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\begin{cases}\n",
    "0 & \\text{if } \\Delta = 0 \\\\\n",
    "60^\\circ \\times \\left( \\frac{G - B}{\\Delta} \\mod 6 \\right) & \\text{if } C_{\\text{max}} = R \\\\\n",
    "60^\\circ \\times \\left( \\frac{B - R}{\\Delta} + 2 \\right) & \\text{if } C_{\\text{max}} = G \\\\\n",
    "60^\\circ \\times \\left( \\frac{R - G}{\\Delta} + 4 \\right) & \\text{if } C_{\\text{max}} = B\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "**‚úçÔ∏è Note:**\n",
    "\n",
    "- The OpenCV implementation of HSV scales:\n",
    "  - **H to [0, 179]** instead of [0¬∞, 360¬∞]\n",
    "  - **S and V to [0, 255]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_5_1_'></a>[Using OpenCV](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_hsv = cv2.cvtColor(im_1, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4), layout=\"compressed\")\n",
    "titles = [\"HSV Image\", \"Hue Channel\", \"Saturation Channel\", \"Value Channel\"]\n",
    "images = [im_1_to_hsv] + [im_1_to_hsv[:, :, i] for i in range(3)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\" if img.ndim == 2 else None, vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_6_'></a>[RGB - CMYK](#toc0_)\n",
    "\n",
    "CMYK stands for **Cyan**, **Magenta**, **Yellow**, and **Key (Black)** ‚Äî a subtractive color model used primarily in color printing.  \n",
    "Unlike RGB (which is additive and designed for light-based displays), CMYK is based on ink or pigment absorption on physical media.\n",
    "\n",
    "- **C (Cyan):** Absorbs red light.\n",
    "- **M (Magenta):** Absorbs green light.\n",
    "- **Y (Yellow):** Absorbs blue light.\n",
    "- **K (Black):** Represents the black component used to enhance depth and reduce ink usage.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 40px; align-items: flex-start; margin: 20px 0;\">\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/RGB_combination_on_wall.png\"\n",
    "         alt=\"RGB_combination_on_wall.png\"\n",
    "         style=\"height: 500px; width: auto; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">\n",
    "      Additive Colors (¬©Ô∏è\n",
    "      <a href=\"https://en.wikipedia.org/wiki/Additive_color\">Wikipedia - Additive color</a>)\n",
    "    </div>\n",
    "  </div>\n",
    "  <div style=\"flex: 1; text-align: center; padding: 10px;\">\n",
    "    <img src=\"../../assets/images/third_party/SubtractiveColor.svg\"\n",
    "         alt=\"SubtractiveColor.svg\"\n",
    "         style=\"height: 500px; width: auto; object-fit: contain;\">\n",
    "    <div style=\"margin-top: 8px;\">\n",
    "      Subtractive Colors (¬©Ô∏è\n",
    "      <a href=\"https://en.wikipedia.org/wiki/Subtractive_color\">Wikipedia - Subtractive color</a>)\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "**Purpose of RGB to CMYK Conversion:**\n",
    "\n",
    "- **Printing:**  \n",
    "  CMYK is the standard color model for printers and press workflows, as it corresponds directly to ink usage.\n",
    "\n",
    "- **Color Separation:**  \n",
    "  The black (K) channel allows better shadow reproduction and text clarity without combining all three color inks.\n",
    "\n",
    "- **Cost Efficiency:**  \n",
    "  Using K instead of combining C, M, and Y for dark tones reduces ink consumption and drying time.\n",
    "\n",
    "üî¢ **RGB to CMYK Conversion (normalized RGB ‚àà [0,1]):**\n",
    "\n",
    "Let $R, G, B \\in [0, 1]$. First, compute the black component:\n",
    "\n",
    "$$\n",
    "K = 1 - \\max(R, G, B)\n",
    "$$\n",
    "\n",
    "Then compute the CMY components:\n",
    "\n",
    "$$\n",
    "C = \\frac{1 - R - K}{1 - K} \\quad\n",
    "M = \\frac{1 - G - K}{1 - K} \\quad\n",
    "Y = \\frac{1 - B - K}{1 - K}\n",
    "$$\n",
    "\n",
    "Special case:\n",
    "\n",
    "- If \\( R = G = B = 0 \\) (pure black), then \\( C = M = Y = 0 \\), \\( K = 1 \\)\n",
    "\n",
    "‚úçÔ∏è **Notes:**\n",
    "\n",
    "- CMYK is **device-dependent**, meaning values may vary depending on printer profiles and paper type.\n",
    "- RGB ‚Üí CMYK is **not reversible** in general, because CMYK has a smaller color gamut.\n",
    "- Normalized values are often scaled to \\([0, 100]\\%\\) or \\([0, 255]\\) for practical implementations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc4_6_1_'></a>[Using PIL](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1_to_cmyk = np.array(Image.fromarray(im_1).convert('CMYK'))\n",
    "\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 4), layout=\"compressed\")\n",
    "titles = [\"CMYK Image\", \"Cyan\", \"Magenta\", \"Yellow\", \"Key\"]\n",
    "images = [Image.fromarray(im_1).convert('CMYK')] + [im_1_to_cmyk[:, :, i] for i in range(4)]\n",
    "for ax, img, title in zip(axs, images, titles):\n",
    "    ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Visualize using Custom Color Maps](#toc0_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_1_'></a>[Indexed Image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch color palletes\n",
    "colors = np.array(image_2.getpalette(), dtype=np.uint8).reshape(-1, 3)\n",
    "num_colors =colors.shape[0]\n",
    "\n",
    "# find alpha index\n",
    "alpha_idx = np.ones(num_colors)\n",
    "if 'transparency' in image_2.info:\n",
    "    t = image_2.info['transparency']\n",
    "    if isinstance(t, bytes):\n",
    "        alpha_idx = np.array(list(t), dtype=np.uint8) / 255.0\n",
    "    elif isinstance(t, int):\n",
    "        alpha_idx[t] = 0.0\n",
    "\n",
    "# add alpha channel + normalization\n",
    "colors_norm = np.concatenate([colors / 255.0, alpha_idx[:, None]], axis=1)\n",
    "\n",
    "# log\n",
    "print(f\"num colors : {num_colors}\")\n",
    "print(f\"alpha idx  : {alpha_idx}\")\n",
    "print(f\"colors:\\n{colors_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a color map\n",
    "cmap = ListedColormap(colors_norm)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4), layout=\"compressed\")\n",
    "axs[0].imshow(im_2, cmap='gray', vmin=0, vmax=im_2.max())\n",
    "axs[0].set_title(\"Wrong cmap\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(im_2, cmap=cmap, vmin=0, vmax=num_colors)\n",
    "axs[1].set_title(\"Correct cmap\")\n",
    "axs[1].axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "media-processing-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
