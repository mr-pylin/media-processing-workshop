{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "üìù **Author:** Amirhossein Heydari - üìß **Email:** <amirhosseinheydari78@gmail.com> - üìç **Origin:** [mr-pylin/media-processing-workshop](https://github.com/mr-pylin/media-processing-workshop)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Table of contents**<a id='toc0_'></a>    \n",
        "- [Dependencies](#toc1_)    \n",
        "- [Load an Image](#toc2_)    \n",
        "  - [Image Degradation](#toc2_1_)    \n",
        "- [Quality Assessment](#toc3_)    \n",
        "  - [Full-Reference (FR-IQA) Metrics](#toc3_1_)    \n",
        "    - [Pixel-Based Metrics](#toc3_1_1_)    \n",
        "      - [Mean Squared Error (MSE)](#toc3_1_1_1_)    \n",
        "        - [Manual](#toc3_1_1_1_1_)    \n",
        "        - [Using scikit-image](#toc3_1_1_1_2_)    \n",
        "      - [Peak Signal-to-Noise Ratio (PSNR)](#toc3_1_1_2_)    \n",
        "        - [Manual](#toc3_1_1_2_1_)    \n",
        "        - [Using scikit-image](#toc3_1_1_2_2_)    \n",
        "    - [Structural and Perceptual Metrics](#toc3_1_2_)    \n",
        "      - [Structural Similarity Index (SSIM)](#toc3_1_2_1_)    \n",
        "        - [Using scikit-image](#toc3_1_2_1_1_)    \n",
        "      - [Multi-Scale Structural Similarity (MS-SSIM)](#toc3_1_2_2_)    \n",
        "        - [Manual](#toc3_1_2_2_1_)    \n",
        "      - [Feature Similarity Index (FSIM)](#toc3_1_2_3_)    \n",
        "      - [Visual Information Fidelity (VIF)](#toc3_1_2_4_)    \n",
        "  - [Reduced-Reference (RR-IQA) Metrics](#toc3_2_)    \n",
        "    - [Reduced-Reference Entropy Difference (RRED)](#toc3_2_1_)    \n",
        "    - [Wavelet-Based Reduced Reference (WBRR)](#toc3_2_2_)    \n",
        "    - [Reduced-Reference Image Quality Assessment (RRIQA)](#toc3_2_3_)    \n",
        "  - [No-Reference (NR-IQA) Metrics](#toc3_3_)    \n",
        "    - [Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)](#toc3_3_1_)    \n",
        "    - [Natural Image Quality Evaluator (NIQE)](#toc3_3_2_)    \n",
        "    - [Perception-based Image Quality Evaluator (PIQE)](#toc3_3_3_)    \n",
        "    - [No-Reference Image Quality Assessment via Transformers (NR-IQA)](#toc3_3_4_)    \n",
        "    - [MetaIQA](#toc3_3_5_)    \n",
        "\n",
        "<!-- vscode-jupyter-toc-config\n",
        "\tnumbering=false\n",
        "\tanchor=true\n",
        "\tflat=false\n",
        "\tminLevel=1\n",
        "\tmaxLevel=6\n",
        "\t/vscode-jupyter-toc-config -->\n",
        "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc1_'></a>[Dependencies](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import skimage\n",
        "from numpy.typing import NDArray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc2_'></a>[Load an Image](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "im_1 = cv2.imread(\"../../assets/images/dip_3rd/CH02_Fig0222(b)(cameraman).tif\", flags=cv2.IMREAD_GRAYSCALE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "plt.imshow(im_1, cmap=\"gray\")\n",
        "plt.title(\"CH02_Fig0222(b)(cameraman).tif\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc2_1_'></a>[Image Degradation](#toc0_)\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- `cv2.filter2D`: [docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# negative\n",
        "max_value = 2 ** np.iinfo(im_1.dtype).bits - 1\n",
        "im_1_negative = max_value - im_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gaussian noise\n",
        "gaussian_noise = np.random.default_rng().normal(loc=0, scale=25, size=im_1.shape)\n",
        "im_1_gaussian_noise = np.clip(im_1.astype(np.float64) + gaussian_noise, 0, 255).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# periodic noise (sinusoidal)\n",
        "amplitude = 50\n",
        "frequency = 10\n",
        "rows, cols = im_1.shape\n",
        "X, Y = np.meshgrid(np.arange(cols), np.arange(rows))\n",
        "periodic_noise = amplitude * np.sin(2 * np.pi * frequency * X / cols + 2 * np.pi * frequency * Y / rows)\n",
        "im_1_periodic_noise = np.clip(im_1.astype(np.float64) + periodic_noise, 0, 255).astype(np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# motion blur\n",
        "size = 5\n",
        "kernel = np.eye(size) / size\n",
        "im_1_blur = cv2.filter2D(im_1, -1, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot\n",
        "titles = [\"Negative\", \"Gaussian Noise\", \"Periodic Noise\", \"Motion Blur\"]\n",
        "images = [im_1_negative, im_1_gaussian_noise, im_1_periodic_noise, im_1_blur]\n",
        "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 4), layout=\"compressed\")\n",
        "for i, ax in enumerate(fig.axes):\n",
        "    ax.imshow(images[i], cmap=\"gray\", vmin=0, vmax=255)\n",
        "    ax.set_title(titles[i])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <a id='toc3_'></a>[Quality Assessment](#toc0_)\n",
        "\n",
        "- It's used to measure the **degradation** caused by various operations like **filtering**, **compression**, **noise**, and **enhancement**.\n",
        "\n",
        "üìù **Docs**:\n",
        "\n",
        "- `skimage.metrics`: [scikit-image.org/docs/stable/api/skimage.metrics.html](https://scikit-image.org/docs/stable/api/skimage.metrics.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc3_1_'></a>[Full-Reference (FR-IQA) Metrics](#toc0_)\n",
        "\n",
        "- These methods compare a **processed** image against a high-quality original (**reference**) image.\n",
        "\n",
        "<table style=\"margin:0 auto;\">\n",
        "  <tr>\n",
        "    <th>Metric</th>\n",
        "    <th>Range</th>\n",
        "    <th>Better Quality</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Mean Squared Error (MSE)</td>\n",
        "    <td>[0, ‚àû)</td>\n",
        "    <td>Lower</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Peak Signal-to-Noise Ratio (PSNR)</td>\n",
        "    <td>[0, ‚àû), typically 30-50 dB</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Structural Similarity Index (SSIM)</td>\n",
        "    <td>[-1, 1], typically [0,1]</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Multi-Scale Structural Similarity (MS-SSIM)</td>\n",
        "    <td>[0, 1]</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Feature Similarity Index (FSIM)</td>\n",
        "    <td>[0, 1]</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Visual Information Fidelity (VIF)</td>\n",
        "    <td>[0, ‚àû), typically [0,1]</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_1_1_'></a>[Pixel-Based Metrics](#toc0_)\n",
        "\n",
        "- Pixel-based metrics directly compare the pixel values of the reference and test images.\n",
        "- They are straightforward and computationally efficient but often do not correlate well with human perception of image quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc3_1_1_1_'></a>[Mean Squared Error (MSE)](#toc0_)\n",
        "\n",
        "- Measures the average squared difference between the original and distorted images.\n",
        "\n",
        "$$\\text{MSE} = \\frac{1}{MN} \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left( I(i,j) - K(i,j) \\right)^2$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <a id='toc3_1_1_1_1_'></a>[Manual](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse(image_1: NDArray, image_2: NDArray) -> float:\n",
        "    return ((image_1.astype(np.float64) - image_2.astype(np.float64)) ** 2).mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"mean-squared-error\")\n",
        "for i in range(len(images)):\n",
        "    print(f\"\\t{titles[i]:{len(max(titles))}} : {mse(im_1, images[i])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <a id='toc3_1_1_1_2_'></a>[Using scikit-image](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"mean-squared-error\")\n",
        "for i in range(len(images)):\n",
        "    print(f\"\\t{titles[i]:{len(max(titles))}} : {skimage.metrics.mean_squared_error(im_1, images[i]).item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc3_1_1_2_'></a>[Peak Signal-to-Noise Ratio (PSNR)](#toc0_)\n",
        "\n",
        "- Derived from MSE, it indicates the ratio of the maximum possible power of a signal to the power of corrupting noise.\n",
        "\n",
        "$$\\text{PSNR} = 10 \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <a id='toc3_1_1_2_1_'></a>[Manual](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def psnr(image_1: NDArray, image_2: NDArray) -> float:\n",
        "    max_value = 2 ** (np.iinfo(image_1.dtype).bits) - 1\n",
        "    mse_value = np.mean((image_1.astype(np.float64) - image_2.astype(np.float64)) ** 2)\n",
        "    if mse_value == 0:\n",
        "        return float(\"inf\")\n",
        "    return 10 * np.log10((max_value**2) / mse_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"peak-signal-noise-ratio\")\n",
        "for i in range(len(images)):\n",
        "    print(f\"\\t{titles[i]:{len(max(titles))}} : {psnr(im_1, images[i])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <a id='toc3_1_1_2_2_'></a>[Using scikit-image](#toc0_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"peak-signal-noise-ratio\")\n",
        "for i in range(len(images)):\n",
        "    print(f\"\\t{titles[i]:{len(max(titles))}} : {skimage.metrics.peak_signal_noise_ratio(im_1, images[i])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_1_2_'></a>[Structural and Perceptual Metrics](#toc0_)\n",
        "\n",
        "- Perceptual metrics aim to model the human visual system (HVS) and assess image quality based on how humans perceive differences between images.\n",
        "-  These metrics consider factors like luminance, contrast, and structural information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc3_1_2_1_'></a>[Structural Similarity Index (SSIM)](#toc0_)\n",
        "\n",
        "- Evaluates image quality based on changes in structural information, luminance, and contrast.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [Image quality assessment: from error visibility to structural similarity](https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <a id='toc3_1_2_1_1_'></a>[Using scikit-image](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"structural-similarity\")\n",
        "for i in range(len(images)):\n",
        "    print(\n",
        "        f\"\\t{titles[i]:{len(max(titles))}} : {skimage.metrics.structural_similarity(im_1, images[i], win_size=11, sigma=1.5, gaussian_weights=True, K1=0.01, K2=0.03)}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc3_1_2_2_'></a>[Multi-Scale Structural Similarity (MS-SSIM)](#toc0_)\n",
        "\n",
        "- It is an extension of the Structural Similarity Index (SSIM) that evaluates image quality across multiple spatial scales.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [Multiscale structural similarity for image quality assessment](https://utw10503.utweb.utexas.edu/publications/2003/zw_asil2003_msssim.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### <a id='toc3_1_2_2_1_'></a>[Manual](#toc0_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gaussian_filter(size=11, sigma=1.5):\n",
        "    x = np.linspace(-size // 2, size // 2, size)\n",
        "    gauss = np.exp(-(x**2) / (2 * sigma**2))\n",
        "    gauss = gauss / gauss.sum()\n",
        "    kernel = np.outer(gauss, gauss)\n",
        "    return kernel\n",
        "\n",
        "\n",
        "def downsample(image):\n",
        "    image = cv2.filter2D(image, -1, gaussian_filter())\n",
        "    return cv2.pyrDown(image)\n",
        "\n",
        "\n",
        "def ms_ssim(img1, img2, levels=5, weight_factors=None):\n",
        "    if weight_factors is None:\n",
        "        weight_factors = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]  # default weights from the paper\n",
        "\n",
        "    msssim = []\n",
        "    epsilon = 1e-8  # small value to prevent numerical instability\n",
        "\n",
        "    for _ in range(levels):\n",
        "        data_range = max(img1.max() - img1.min(), epsilon)  # avoid zero data range\n",
        "        score, _ = skimage.metrics.structural_similarity(img1, img2, full=True, data_range=data_range)\n",
        "\n",
        "        score = np.fabs(score)  # use absolute value instead of clamping\n",
        "        msssim.append(score)\n",
        "\n",
        "        img1, img2 = downsample(img1), downsample(img2)\n",
        "\n",
        "    return np.prod(np.array(msssim) ** np.array(weight_factors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"multi-scale-structural-similarity\")\n",
        "for i in range(len(images)):\n",
        "    print(f\"\\t{titles[i]:{len(max(titles))}} : {ms_ssim(im_1, images[i])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc3_1_2_3_'></a>[Feature Similarity Index (FSIM)](#toc0_)\n",
        "\n",
        "- Uses phase congruency and gradient magnitude to assess image quality.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [FSIM: A Feature Similarity Index for Image Quality Assessment](https://ieeexplore.ieee.org/abstract/document/5705575)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### <a id='toc3_1_2_4_'></a>[Visual Information Fidelity (VIF)](#toc0_)\n",
        "\n",
        "- Measures the amount of information that can be extracted by the human visual system from the distorted image relative to the reference image.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [Image information and visual quality](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=58d0d3b905c6531e25d94b8c20605a0e1e1ba1cb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc3_2_'></a>[Reduced-Reference (RR-IQA) Metrics](#toc0_)\n",
        "\n",
        "- These methods require only **partial** information from the **reference** image.\n",
        "\n",
        "<table style=\"margin:0 auto;\">\n",
        "  <tr>\n",
        "    <th>Metric</th>\n",
        "    <th>Range</th>\n",
        "    <th>Better Quality</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Reduced-Reference Entropy Difference (RRED)</td>\n",
        "    <td>[0, ‚àû)</td>\n",
        "    <td>Lower</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Wavelet-Based Reduced Reference (WBRR)</td>\n",
        "    <td>[0, 1]</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>Reduced-Reference Image Quality Assessment (RRIQA)</td>\n",
        "    <td>[0, 1]</td>\n",
        "    <td>Higher</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_2_1_'></a>[Reduced-Reference Entropy Difference (RRED)](#toc0_)\n",
        "\n",
        "- Uses entropy differences between the reference and distorted images.\n",
        "\n",
        "üìù **Paper**:\n",
        "\n",
        "- [RRED Indices: Reduced Reference Entropic Differencing for Image Quality Assessment](https://ieeexplore.ieee.org/document/5999718)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_2_2_'></a>[Wavelet-Based Reduced Reference (WBRR)](#toc0_)\n",
        "\n",
        "- Utilizes wavelet transform coefficients to compare the reference and distorted images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_2_3_'></a>[Reduced-Reference Image Quality Assessment (RRIQA)](#toc0_)\n",
        "\n",
        "- Employs natural scene statistics in the wavelet domain to evaluate image quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## <a id='toc3_3_'></a>[No-Reference (NR-IQA) Metrics](#toc0_)\n",
        "\n",
        "- These methods work **without** a **reference** image.\n",
        "\n",
        "<table style=\"margin:0 auto;\">\n",
        "  <tr>\n",
        "    <th>Metric</th>\n",
        "    <th>Range</th>\n",
        "    <th>Better Quality</th>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator)</td>\n",
        "    <td>[0, 100]</td>\n",
        "    <td>Lower</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>NIQE (Natural Image Quality Evaluator)</td>\n",
        "    <td>[0, ‚àû), typically [0, 10]</td>\n",
        "    <td>Lower</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>PIQE (Perception-based Image Quality Evaluator)</td>\n",
        "    <td>[0, 100]</td>\n",
        "    <td>Lower</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>No-Reference Image Quality Assessment via Transformers (NR-IQA)</td>\n",
        "    <td>Depends on implementation</td>\n",
        "    <td>Varies</td>\n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td>MetaIQA</td>\n",
        "    <td>Depends on implementation</td>\n",
        "    <td>Varies</td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_3_1_'></a>[Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)](#toc0_)\n",
        "\n",
        "- Uses natural scene statistics to evaluate image quality (**Requires Pretrained Model**).\n",
        "\n",
        "**Pretrained Models**:\n",
        "\n",
        "- `brisque_model_live.yml`: [github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_model_live.yml](https://github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_model_live.yml)\n",
        "- `brisque_range_live.yml`: [github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_range_live.yml](https://github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_range_live.yml)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_3_2_'></a>[Natural Image Quality Evaluator (NIQE)](#toc0_)\n",
        "\n",
        "- Measures deviations from statistical regularities observed in natural images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_3_3_'></a>[Perception-based Image Quality Evaluator (PIQE)](#toc0_)\n",
        "\n",
        "- Computes a perceptual quality score based on block-wise distortion in the image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_3_4_'></a>[No-Reference Image Quality Assessment via Transformers (NR-IQA)](#toc0_)\n",
        "\n",
        "- Leverages deep learning models like CNNs and Transformers to predict image quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <a id='toc3_3_5_'></a>[MetaIQA](#toc0_)\n",
        "\n",
        "- Uses deep meta-learning to adapt to various distortions and assess image quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "author_email": "AmirhosseinHeydari78@gmail.com",
    "author_github": "https://github.com/mr-pylin",
    "author_name": "Amirhossein Heydari",
    "kernelspec": {
      "display_name": "media-processing-workshop-sxUc00b2-py3.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
