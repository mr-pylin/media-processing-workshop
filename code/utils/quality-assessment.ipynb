{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between; align-items: center;\">\n",
    "    <div style=\"text-align: left; flex: 4;\">\n",
    "        <strong>Author:</strong> Amirhossein Heydari ‚Äî \n",
    "        üìß <a href=\"mailto:amirhosseinheydari78@gmail.com\">amirhosseinheydari78@gmail.com</a> ‚Äî \n",
    "        üêô <a href=\"https://github.com/mr-pylin/media-processing-workshop\" target=\"_blank\" rel=\"noopener\">github.com/mr-pylin</a>\n",
    "    </div>\n",
    "    <div style=\"display: flex; justify-content: flex-end; flex: 1; gap: 8px; align-items: center; padding: 0;\">\n",
    "        <a href=\"https://opencv.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/opencv/logo/OpenCV_logo_no_text-1.svg\"\n",
    "                 alt=\"OpenCV Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://pillow.readthedocs.io/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/pillow/logo/pillow-logo-248x250.png\"\n",
    "                 alt=\"PIL Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scikit-image.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/scikit-image/logo/logo.png\"\n",
    "                 alt=\"scikit-image Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "        <a href=\"https://scipy.org/\" target=\"_blank\" rel=\"noopener noreferrer\">\n",
    "            <img src=\"../../assets/images/libraries/scipy/logo/logo.svg\"\n",
    "                 alt=\"SciPy Logo\"\n",
    "                 style=\"max-height: 48px; width: auto;\">\n",
    "        </a>\n",
    "    </div>\n",
    "</div>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Dependencies](#toc1_)    \n",
    "- [Load an Image](#toc2_)    \n",
    "  - [Image Degradation](#toc2_1_)    \n",
    "- [Quality Assessment](#toc3_)    \n",
    "  - [Full-Reference (FR-IQA) Metrics](#toc3_1_)    \n",
    "    - [Pixel-Based Metrics](#toc3_1_1_)    \n",
    "      - [Mean Squared Error (MSE)](#toc3_1_1_1_)    \n",
    "        - [Manual](#toc3_1_1_1_1_)    \n",
    "        - [Using scikit-image](#toc3_1_1_1_2_)    \n",
    "      - [Peak Signal-to-Noise Ratio (PSNR)](#toc3_1_1_2_)    \n",
    "        - [Manual](#toc3_1_1_2_1_)    \n",
    "        - [Using scikit-image](#toc3_1_1_2_2_)    \n",
    "    - [Structural and Perceptual Metrics](#toc3_1_2_)    \n",
    "      - [Structural Similarity Index (SSIM)](#toc3_1_2_1_)    \n",
    "        - [Using scikit-image](#toc3_1_2_1_1_)    \n",
    "      - [Multi-Scale Structural Similarity (MS-SSIM)](#toc3_1_2_2_)    \n",
    "        - [Manual](#toc3_1_2_2_1_)    \n",
    "      - [Feature Similarity Index (FSIM)](#toc3_1_2_3_)    \n",
    "      - [Visual Information Fidelity (VIF)](#toc3_1_2_4_)    \n",
    "  - [Reduced-Reference (RR-IQA) Metrics](#toc3_2_)    \n",
    "    - [Reduced-Reference Entropy Difference (RRED)](#toc3_2_1_)    \n",
    "    - [Wavelet-Based Reduced Reference (WBRR)](#toc3_2_2_)    \n",
    "    - [Reduced-Reference Image Quality Assessment (RRIQA)](#toc3_2_3_)    \n",
    "  - [No-Reference (NR-IQA) Metrics](#toc3_3_)    \n",
    "    - [Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)](#toc3_3_1_)    \n",
    "    - [Natural Image Quality Evaluator (NIQE)](#toc3_3_2_)    \n",
    "    - [Perception-based Image Quality Evaluator (PIQE)](#toc3_3_3_)    \n",
    "    - [No-Reference Image Quality Assessment via Transformers (NR-IQA)](#toc3_3_4_)    \n",
    "    - [MetaIQA](#toc3_3_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Dependencies](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skimage\n",
    "from numpy.typing import NDArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable automatic figure display (plt.show() required)  \n",
    "# this ensures consistency with .py scripts and gives full control over when plots appear\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Load an Image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_1 = cv2.imread(\"../../assets/images/dip_3rd/CH02_Fig0222(b)(cameraman).tif\", flags=cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.imshow(im_1, cmap=\"gray\")\n",
    "plt.title(\"CH02_Fig0222(b)(cameraman).tif\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Image Degradation](#toc0_)\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `cv2.filter2D`: [docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative\n",
    "max_value = 2 ** np.iinfo(im_1.dtype).bits - 1\n",
    "im_1_negative = max_value - im_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian noise\n",
    "gaussian_noise = np.random.default_rng(seed=42).normal(loc=0, scale=25, size=im_1.shape)\n",
    "im_1_gaussian_noise = np.clip(im_1.astype(np.float64) + gaussian_noise, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# periodic noise (sinusoidal)\n",
    "amplitude = 50\n",
    "frequency = 10\n",
    "rows, cols = im_1.shape\n",
    "X, Y = np.meshgrid(np.arange(cols), np.arange(rows))\n",
    "periodic_noise = amplitude * np.sin(2 * np.pi * frequency * X / cols + 2 * np.pi * frequency * Y / rows)\n",
    "im_1_periodic_noise = np.clip(im_1.astype(np.float64) + periodic_noise, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# motion blur\n",
    "size = 5\n",
    "kernel = np.eye(size) / size\n",
    "im_1_blur = cv2.filter2D(im_1, -1, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "titles = [\"Negative\", \"Gaussian Noise\", \"Periodic Noise\", \"Motion Blur\"]\n",
    "images = [im_1_negative, im_1_gaussian_noise, im_1_periodic_noise, im_1_blur]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(16, 4), layout=\"compressed\")\n",
    "for i, ax in enumerate(fig.axes):\n",
    "    ax.imshow(images[i], cmap=\"gray\", vmin=0, vmax=255)\n",
    "    ax.set_title(titles[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Quality Assessment](#toc0_)\n",
    "\n",
    "- It's used to measure the **degradation** caused by various operations like **filtering**, **compression**, **noise**, and **enhancement**.\n",
    "\n",
    "üìù **Docs**:\n",
    "\n",
    "- `skimage.metrics`: [scikit-image.org/docs/stable/api/skimage.metrics.html](https://scikit-image.org/docs/stable/api/skimage.metrics.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Full-Reference (FR-IQA) Metrics](#toc0_)\n",
    "\n",
    "- These methods compare a **processed** image against a high-quality original (**reference**) image.\n",
    "\n",
    "<table style=\"margin:0 auto;\">\n",
    "  <tr>\n",
    "    <th>Metric</th>\n",
    "    <th>Range</th>\n",
    "    <th>Better Quality</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mean Squared Error (MSE)</td>\n",
    "    <td>[0, ‚àû)</td>\n",
    "    <td>Lower</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Peak Signal-to-Noise Ratio (PSNR)</td>\n",
    "    <td>[0, ‚àû), typically 30-50 dB</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Structural Similarity Index (SSIM)</td>\n",
    "    <td>[-1, 1], typically [0,1]</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Multi-Scale Structural Similarity (MS-SSIM)</td>\n",
    "    <td>[0, 1]</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Feature Similarity Index (FSIM)</td>\n",
    "    <td>[0, 1]</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Visual Information Fidelity (VIF)</td>\n",
    "    <td>[0, ‚àû), typically [0,1]</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_1_'></a>[Pixel-Based Metrics](#toc0_)\n",
    "\n",
    "- Pixel-based metrics directly compare the pixel values of the reference and test images.\n",
    "- They are straightforward and computationally efficient but often do not correlate well with human perception of image quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_1_'></a>[Mean Squared Error (MSE)](#toc0_)\n",
    "\n",
    "- Measures the average squared difference between the original and distorted images.\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{MN} \\sum_{i=1}^{M} \\sum_{j=1}^{N} \\left( I(i,j) - K(i,j) \\right)^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc3_1_1_1_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(image_1: NDArray, image_2: NDArray) -> float:\n",
    "    return ((image_1.astype(np.float64) - image_2.astype(np.float64)) ** 2).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean-squared-error\")\n",
    "for i in range(len(images)):\n",
    "    print(f\"\\t{titles[i]:{len(max(titles))}} : {mse(im_1, images[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc3_1_1_1_2_'></a>[Using scikit-image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean-squared-error\")\n",
    "for i in range(len(images)):\n",
    "    print(f\"\\t{titles[i]:{len(max(titles))}} : {skimage.metrics.mean_squared_error(im_1, images[i]).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_1_2_'></a>[Peak Signal-to-Noise Ratio (PSNR)](#toc0_)\n",
    "\n",
    "- Derived from MSE, it indicates the ratio of the maximum possible power of a signal to the power of corrupting noise.\n",
    "\n",
    "$$\\text{PSNR} = 10 \\log_{10} \\left( \\frac{\\text{MAX}^2}{\\text{MSE}} \\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc3_1_1_2_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(image_1: NDArray, image_2: NDArray) -> float:\n",
    "    max_value = 2 ** (np.iinfo(image_1.dtype).bits) - 1\n",
    "    mse_value = np.mean((image_1.astype(np.float64) - image_2.astype(np.float64)) ** 2)\n",
    "    if mse_value == 0:\n",
    "        return float(\"inf\")\n",
    "    return 10 * np.log10((max_value**2) / mse_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"peak-signal-noise-ratio\")\n",
    "for i in range(len(images)):\n",
    "    print(f\"\\t{titles[i]:{len(max(titles))}} : {psnr(im_1, images[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc3_1_1_2_2_'></a>[Using scikit-image](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"peak-signal-noise-ratio\")\n",
    "for i in range(len(images)):\n",
    "    print(f\"\\t{titles[i]:{len(max(titles))}} : {skimage.metrics.peak_signal_noise_ratio(im_1, images[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_1_2_'></a>[Structural and Perceptual Metrics](#toc0_)\n",
    "\n",
    "- Perceptual metrics aim to model the human visual system (HVS) and assess image quality based on how humans perceive differences between images.\n",
    "-  These metrics consider factors like luminance, contrast, and structural information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_1_'></a>[Structural Similarity Index (SSIM)](#toc0_)\n",
    "\n",
    "- Evaluates image quality based on changes in structural information, luminance, and contrast.\n",
    "\n",
    "üìù **Paper**:\n",
    "\n",
    "- [Image quality assessment: from error visibility to structural similarity](https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc3_1_2_1_1_'></a>[Using scikit-image](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"structural-similarity\")\n",
    "for i in range(len(images)):\n",
    "    print(\n",
    "        f\"\\t{titles[i]:{len(max(titles))}} : {skimage.metrics.structural_similarity(im_1, images[i], win_size=11, sigma=1.5, gaussian_weights=True, K1=0.01, K2=0.03)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_2_'></a>[Multi-Scale Structural Similarity (MS-SSIM)](#toc0_)\n",
    "\n",
    "- It is an extension of the Structural Similarity Index (SSIM) that evaluates image quality across multiple spatial scales.\n",
    "\n",
    "üìù **Paper**:\n",
    "\n",
    "- [Multiscale structural similarity for image quality assessment](https://utw10503.utweb.utexas.edu/publications/2003/zw_asil2003_msssim.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <a id='toc3_1_2_2_1_'></a>[Manual](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(size=11, sigma=1.5):\n",
    "    x = np.linspace(-size // 2, size // 2, size)\n",
    "    gauss = np.exp(-(x**2) / (2 * sigma**2))\n",
    "    gauss = gauss / gauss.sum()\n",
    "    kernel = np.outer(gauss, gauss)\n",
    "    return kernel\n",
    "\n",
    "\n",
    "def downsample(image):\n",
    "    image = cv2.filter2D(image, -1, gaussian_filter())\n",
    "    return cv2.pyrDown(image)\n",
    "\n",
    "\n",
    "def ms_ssim(img1, img2, levels=5, weight_factors=None):\n",
    "    if weight_factors is None:\n",
    "        weight_factors = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]  # default weights from the paper\n",
    "\n",
    "    msssim = []\n",
    "    epsilon = 1e-8  # small value to prevent numerical instability\n",
    "\n",
    "    for _ in range(levels):\n",
    "        data_range = max(img1.max() - img1.min(), epsilon)  # avoid zero data range\n",
    "        score, _ = skimage.metrics.structural_similarity(img1, img2, full=True, data_range=data_range)\n",
    "\n",
    "        score = np.fabs(score)  # use absolute value instead of clamping\n",
    "        msssim.append(score)\n",
    "\n",
    "        img1, img2 = downsample(img1), downsample(img2)\n",
    "\n",
    "    return np.prod(np.array(msssim) ** np.array(weight_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"multi-scale-structural-similarity\")\n",
    "for i in range(len(images)):\n",
    "    print(f\"\\t{titles[i]:{len(max(titles))}} : {ms_ssim(im_1, images[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_3_'></a>[Feature Similarity Index (FSIM)](#toc0_)\n",
    "\n",
    "- Uses phase congruency and gradient magnitude to assess image quality.\n",
    "\n",
    "üìù **Paper**:\n",
    "\n",
    "- [FSIM: A Feature Similarity Index for Image Quality Assessment](https://ieeexplore.ieee.org/abstract/document/5705575)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id='toc3_1_2_4_'></a>[Visual Information Fidelity (VIF)](#toc0_)\n",
    "\n",
    "- Measures the amount of information that can be extracted by the human visual system from the distorted image relative to the reference image.\n",
    "\n",
    "üìù **Paper**:\n",
    "\n",
    "- [Image information and visual quality](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=58d0d3b905c6531e25d94b8c20605a0e1e1ba1cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Reduced-Reference (RR-IQA) Metrics](#toc0_)\n",
    "\n",
    "- These methods require only **partial** information from the **reference** image.\n",
    "\n",
    "<table style=\"margin:0 auto;\">\n",
    "  <tr>\n",
    "    <th>Metric</th>\n",
    "    <th>Range</th>\n",
    "    <th>Better Quality</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Reduced-Reference Entropy Difference (RRED)</td>\n",
    "    <td>[0, ‚àû)</td>\n",
    "    <td>Lower</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wavelet-Based Reduced Reference (WBRR)</td>\n",
    "    <td>[0, 1]</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Reduced-Reference Image Quality Assessment (RRIQA)</td>\n",
    "    <td>[0, 1]</td>\n",
    "    <td>Higher</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_1_'></a>[Reduced-Reference Entropy Difference (RRED)](#toc0_)\n",
    "\n",
    "- Uses entropy differences between the reference and distorted images.\n",
    "\n",
    "üìù **Paper**:\n",
    "\n",
    "- [RRED Indices: Reduced Reference Entropic Differencing for Image Quality Assessment](https://ieeexplore.ieee.org/document/5999718)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_2_'></a>[Wavelet-Based Reduced Reference (WBRR)](#toc0_)\n",
    "\n",
    "- Utilizes wavelet transform coefficients to compare the reference and distorted images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_2_3_'></a>[Reduced-Reference Image Quality Assessment (RRIQA)](#toc0_)\n",
    "\n",
    "- Employs natural scene statistics in the wavelet domain to evaluate image quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[No-Reference (NR-IQA) Metrics](#toc0_)\n",
    "\n",
    "- These methods work **without** a **reference** image.\n",
    "\n",
    "<table style=\"margin:0 auto;\">\n",
    "  <tr>\n",
    "    <th>Metric</th>\n",
    "    <th>Range</th>\n",
    "    <th>Better Quality</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BRISQUE (Blind/Referenceless Image Spatial Quality Evaluator)</td>\n",
    "    <td>[0, 100]</td>\n",
    "    <td>Lower</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>NIQE (Natural Image Quality Evaluator)</td>\n",
    "    <td>[0, ‚àû), typically [0, 10]</td>\n",
    "    <td>Lower</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>PIQE (Perception-based Image Quality Evaluator)</td>\n",
    "    <td>[0, 100]</td>\n",
    "    <td>Lower</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No-Reference Image Quality Assessment via Transformers (NR-IQA)</td>\n",
    "    <td>Depends on implementation</td>\n",
    "    <td>Varies</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>MetaIQA</td>\n",
    "    <td>Depends on implementation</td>\n",
    "    <td>Varies</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_1_'></a>[Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)](#toc0_)\n",
    "\n",
    "- Uses natural scene statistics to evaluate image quality (**Requires Pretrained Model**).\n",
    "\n",
    "**Pretrained Models**:\n",
    "\n",
    "- `brisque_model_live.yml`: [github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_model_live.yml](https://github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_model_live.yml)\n",
    "- `brisque_range_live.yml`: [github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_range_live.yml](https://github.com/opencv/opencv_contrib/blob/master/modules/quality/samples/brisque_range_live.yml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_2_'></a>[Natural Image Quality Evaluator (NIQE)](#toc0_)\n",
    "\n",
    "- Measures deviations from statistical regularities observed in natural images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_3_'></a>[Perception-based Image Quality Evaluator (PIQE)](#toc0_)\n",
    "\n",
    "- Computes a perceptual quality score based on block-wise distortion in the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_4_'></a>[No-Reference Image Quality Assessment via Transformers (NR-IQA)](#toc0_)\n",
    "\n",
    "- Leverages deep learning models like CNNs and Transformers to predict image quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc3_3_5_'></a>[MetaIQA](#toc0_)\n",
    "\n",
    "- Uses deep meta-learning to adapt to various distortions and assess image quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "author_email": "AmirhosseinHeydari78@gmail.com",
  "author_github": "https://github.com/mr-pylin",
  "author_name": "Amirhossein Heydari",
  "kernelspec": {
   "display_name": "media-processing-workshop (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "origin_repo": "https://github.com/mr-pylin/media-processing-workshop"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
